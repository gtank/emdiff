{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 50,
  "global_step": 171,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 8.45615005493164,
      "learning_rate": 0.0,
      "loss": 2.6445,
      "step": 1
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 8.765111923217773,
      "learning_rate": 1e-05,
      "loss": 2.6298,
      "step": 2
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 7.719338893890381,
      "learning_rate": 2e-05,
      "loss": 2.5972,
      "step": 3
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 7.93421745300293,
      "learning_rate": 3e-05,
      "loss": 2.4791,
      "step": 4
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 7.067592620849609,
      "learning_rate": 4e-05,
      "loss": 2.2903,
      "step": 5
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 6.229429244995117,
      "learning_rate": 5e-05,
      "loss": 2.2851,
      "step": 6
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 4.726749897003174,
      "learning_rate": 6e-05,
      "loss": 2.063,
      "step": 7
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 4.089012145996094,
      "learning_rate": 7e-05,
      "loss": 2.0109,
      "step": 8
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.7058143615722656,
      "learning_rate": 8e-05,
      "loss": 1.8361,
      "step": 9
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 3.3792388439178467,
      "learning_rate": 9e-05,
      "loss": 1.7605,
      "step": 10
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 3.5232701301574707,
      "learning_rate": 0.0001,
      "loss": 1.6887,
      "step": 11
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 3.227825880050659,
      "learning_rate": 9.937888198757764e-05,
      "loss": 1.6383,
      "step": 12
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 2.9552969932556152,
      "learning_rate": 9.875776397515528e-05,
      "loss": 1.6022,
      "step": 13
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 3.046355962753296,
      "learning_rate": 9.813664596273293e-05,
      "loss": 1.5801,
      "step": 14
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.7059011459350586,
      "learning_rate": 9.751552795031056e-05,
      "loss": 1.5117,
      "step": 15
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 2.613953113555908,
      "learning_rate": 9.689440993788821e-05,
      "loss": 1.4806,
      "step": 16
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 2.5590178966522217,
      "learning_rate": 9.627329192546585e-05,
      "loss": 1.5085,
      "step": 17
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.3161826133728027,
      "learning_rate": 9.565217391304348e-05,
      "loss": 1.4705,
      "step": 18
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 2.300342321395874,
      "learning_rate": 9.503105590062112e-05,
      "loss": 1.4471,
      "step": 19
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.2360610961914062,
      "learning_rate": 9.440993788819877e-05,
      "loss": 1.3871,
      "step": 20
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.9937413930892944,
      "learning_rate": 9.37888198757764e-05,
      "loss": 1.3722,
      "step": 21
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 2.0586774349212646,
      "learning_rate": 9.316770186335404e-05,
      "loss": 1.3785,
      "step": 22
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 1.8929049968719482,
      "learning_rate": 9.254658385093168e-05,
      "loss": 1.2946,
      "step": 23
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.9096531867980957,
      "learning_rate": 9.192546583850931e-05,
      "loss": 1.2613,
      "step": 24
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 2.0004794597625732,
      "learning_rate": 9.130434782608696e-05,
      "loss": 1.3118,
      "step": 25
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 2.1952762603759766,
      "learning_rate": 9.068322981366461e-05,
      "loss": 1.236,
      "step": 26
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.016248941421509,
      "learning_rate": 9.006211180124225e-05,
      "loss": 1.2255,
      "step": 27
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 2.098484992980957,
      "learning_rate": 8.944099378881988e-05,
      "loss": 1.2415,
      "step": 28
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 2.2563345432281494,
      "learning_rate": 8.881987577639752e-05,
      "loss": 1.2437,
      "step": 29
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.1959493160247803,
      "learning_rate": 8.819875776397516e-05,
      "loss": 1.2562,
      "step": 30
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 2.1617231369018555,
      "learning_rate": 8.757763975155279e-05,
      "loss": 1.196,
      "step": 31
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 2.2013823986053467,
      "learning_rate": 8.695652173913044e-05,
      "loss": 1.2018,
      "step": 32
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 2.066349983215332,
      "learning_rate": 8.633540372670808e-05,
      "loss": 1.1605,
      "step": 33
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 2.0575435161590576,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.1408,
      "step": 34
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.1692287921905518,
      "learning_rate": 8.509316770186336e-05,
      "loss": 1.1911,
      "step": 35
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.0692148208618164,
      "learning_rate": 8.4472049689441e-05,
      "loss": 1.1761,
      "step": 36
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 2.0591866970062256,
      "learning_rate": 8.385093167701863e-05,
      "loss": 1.153,
      "step": 37
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 2.0472843647003174,
      "learning_rate": 8.322981366459628e-05,
      "loss": 1.1505,
      "step": 38
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 2.1721205711364746,
      "learning_rate": 8.260869565217392e-05,
      "loss": 1.145,
      "step": 39
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 2.1787240505218506,
      "learning_rate": 8.198757763975156e-05,
      "loss": 1.0882,
      "step": 40
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 1.8916476964950562,
      "learning_rate": 8.136645962732919e-05,
      "loss": 1.0704,
      "step": 41
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 1.8859070539474487,
      "learning_rate": 8.074534161490683e-05,
      "loss": 1.0894,
      "step": 42
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 1.8892050981521606,
      "learning_rate": 8.012422360248448e-05,
      "loss": 1.0437,
      "step": 43
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 1.967289686203003,
      "learning_rate": 7.950310559006211e-05,
      "loss": 1.124,
      "step": 44
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.8719499111175537,
      "learning_rate": 7.888198757763976e-05,
      "loss": 1.1185,
      "step": 45
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 1.908286452293396,
      "learning_rate": 7.82608695652174e-05,
      "loss": 1.0561,
      "step": 46
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 1.8525570631027222,
      "learning_rate": 7.763975155279503e-05,
      "loss": 1.0906,
      "step": 47
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 1.9005582332611084,
      "learning_rate": 7.701863354037267e-05,
      "loss": 1.0654,
      "step": 48
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 1.891696572303772,
      "learning_rate": 7.639751552795032e-05,
      "loss": 1.0817,
      "step": 49
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 2.0127341747283936,
      "learning_rate": 7.577639751552796e-05,
      "loss": 1.041,
      "step": 50
    },
    {
      "epoch": 0.8888888888888888,
      "eval_loss": 6.662570953369141,
      "eval_runtime": 43.3347,
      "eval_samples_per_second": 2.308,
      "eval_steps_per_second": 0.577,
      "step": 50
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.8428504467010498,
      "learning_rate": 7.515527950310559e-05,
      "loss": 1.0245,
      "step": 51
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 1.9173622131347656,
      "learning_rate": 7.453416149068323e-05,
      "loss": 1.0255,
      "step": 52
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 1.9616808891296387,
      "learning_rate": 7.391304347826086e-05,
      "loss": 1.0592,
      "step": 53
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.8472869396209717,
      "learning_rate": 7.329192546583851e-05,
      "loss": 1.0558,
      "step": 54
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.9807740449905396,
      "learning_rate": 7.267080745341616e-05,
      "loss": 1.059,
      "step": 55
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 1.8798493146896362,
      "learning_rate": 7.20496894409938e-05,
      "loss": 1.0674,
      "step": 56
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.5839340686798096,
      "learning_rate": 7.142857142857143e-05,
      "loss": 1.0545,
      "step": 57
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 1.7372835874557495,
      "learning_rate": 7.080745341614907e-05,
      "loss": 0.9771,
      "step": 58
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 1.6578129529953003,
      "learning_rate": 7.01863354037267e-05,
      "loss": 1.0105,
      "step": 59
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 1.9548039436340332,
      "learning_rate": 6.956521739130436e-05,
      "loss": 1.0031,
      "step": 60
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 1.8276102542877197,
      "learning_rate": 6.894409937888199e-05,
      "loss": 0.962,
      "step": 61
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 1.8652920722961426,
      "learning_rate": 6.832298136645963e-05,
      "loss": 1.0119,
      "step": 62
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 2.110280752182007,
      "learning_rate": 6.770186335403726e-05,
      "loss": 1.0315,
      "step": 63
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 1.7958004474639893,
      "learning_rate": 6.708074534161491e-05,
      "loss": 0.9817,
      "step": 64
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 1.7580384016036987,
      "learning_rate": 6.645962732919255e-05,
      "loss": 0.991,
      "step": 65
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.8487011194229126,
      "learning_rate": 6.58385093167702e-05,
      "loss": 0.9868,
      "step": 66
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 2.353959083557129,
      "learning_rate": 6.521739130434783e-05,
      "loss": 1.0488,
      "step": 67
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 2.1118781566619873,
      "learning_rate": 6.459627329192547e-05,
      "loss": 1.024,
      "step": 68
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 1.7521933317184448,
      "learning_rate": 6.39751552795031e-05,
      "loss": 0.9889,
      "step": 69
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 1.8270297050476074,
      "learning_rate": 6.335403726708074e-05,
      "loss": 0.992,
      "step": 70
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 1.7979167699813843,
      "learning_rate": 6.273291925465838e-05,
      "loss": 0.9913,
      "step": 71
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.8421214818954468,
      "learning_rate": 6.211180124223603e-05,
      "loss": 1.0159,
      "step": 72
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 1.9525632858276367,
      "learning_rate": 6.149068322981368e-05,
      "loss": 0.9674,
      "step": 73
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 1.779105544090271,
      "learning_rate": 6.086956521739131e-05,
      "loss": 1.0022,
      "step": 74
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.6966198682785034,
      "learning_rate": 6.024844720496895e-05,
      "loss": 0.9933,
      "step": 75
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 1.6293652057647705,
      "learning_rate": 5.962732919254659e-05,
      "loss": 0.9866,
      "step": 76
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 1.6121575832366943,
      "learning_rate": 5.900621118012423e-05,
      "loss": 0.9667,
      "step": 77
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 1.8049508333206177,
      "learning_rate": 5.838509316770186e-05,
      "loss": 0.9518,
      "step": 78
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 1.7818589210510254,
      "learning_rate": 5.7763975155279506e-05,
      "loss": 0.9568,
      "step": 79
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 1.7561365365982056,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.9657,
      "step": 80
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 1.8560073375701904,
      "learning_rate": 5.652173913043478e-05,
      "loss": 1.0087,
      "step": 81
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 2.0350306034088135,
      "learning_rate": 5.590062111801242e-05,
      "loss": 0.9447,
      "step": 82
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 1.8685500621795654,
      "learning_rate": 5.527950310559007e-05,
      "loss": 1.0097,
      "step": 83
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.699385404586792,
      "learning_rate": 5.4658385093167706e-05,
      "loss": 0.9864,
      "step": 84
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 1.7514160871505737,
      "learning_rate": 5.403726708074535e-05,
      "loss": 0.9654,
      "step": 85
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 1.6460801362991333,
      "learning_rate": 5.3416149068322984e-05,
      "loss": 0.9176,
      "step": 86
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1.6779972314834595,
      "learning_rate": 5.279503105590062e-05,
      "loss": 0.9359,
      "step": 87
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 1.6955162286758423,
      "learning_rate": 5.217391304347826e-05,
      "loss": 0.9606,
      "step": 88
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 1.6627675294876099,
      "learning_rate": 5.15527950310559e-05,
      "loss": 0.9414,
      "step": 89
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.7164387702941895,
      "learning_rate": 5.093167701863354e-05,
      "loss": 0.9596,
      "step": 90
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 1.6225719451904297,
      "learning_rate": 5.031055900621118e-05,
      "loss": 0.9461,
      "step": 91
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 1.7902151346206665,
      "learning_rate": 4.968944099378882e-05,
      "loss": 0.994,
      "step": 92
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.661143183708191,
      "learning_rate": 4.906832298136646e-05,
      "loss": 0.9321,
      "step": 93
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 1.7596569061279297,
      "learning_rate": 4.8447204968944106e-05,
      "loss": 0.9538,
      "step": 94
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 1.8920726776123047,
      "learning_rate": 4.782608695652174e-05,
      "loss": 0.9496,
      "step": 95
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 1.8908110857009888,
      "learning_rate": 4.7204968944099384e-05,
      "loss": 0.9717,
      "step": 96
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 1.7174822092056274,
      "learning_rate": 4.658385093167702e-05,
      "loss": 0.9331,
      "step": 97
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 1.7310092449188232,
      "learning_rate": 4.5962732919254656e-05,
      "loss": 0.9948,
      "step": 98
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 1.8559938669204712,
      "learning_rate": 4.5341614906832306e-05,
      "loss": 0.9391,
      "step": 99
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 2.019185781478882,
      "learning_rate": 4.472049689440994e-05,
      "loss": 0.9161,
      "step": 100
    },
    {
      "epoch": 1.7644444444444445,
      "eval_loss": 7.514865875244141,
      "eval_runtime": 1.2564,
      "eval_samples_per_second": 79.592,
      "eval_steps_per_second": 19.898,
      "step": 100
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 1.672957420349121,
      "learning_rate": 4.409937888198758e-05,
      "loss": 0.9783,
      "step": 101
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.0207290649414062,
      "learning_rate": 4.347826086956522e-05,
      "loss": 0.8994,
      "step": 102
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 1.797785997390747,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.9769,
      "step": 103
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 1.8215739727020264,
      "learning_rate": 4.22360248447205e-05,
      "loss": 0.9507,
      "step": 104
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 1.8407577276229858,
      "learning_rate": 4.161490683229814e-05,
      "loss": 0.9931,
      "step": 105
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 1.7545280456542969,
      "learning_rate": 4.099378881987578e-05,
      "loss": 0.9814,
      "step": 106
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.8149751424789429,
      "learning_rate": 4.0372670807453414e-05,
      "loss": 0.971,
      "step": 107
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 1.703123927116394,
      "learning_rate": 3.9751552795031056e-05,
      "loss": 0.943,
      "step": 108
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 1.7347288131713867,
      "learning_rate": 3.91304347826087e-05,
      "loss": 0.9641,
      "step": 109
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 1.7269905805587769,
      "learning_rate": 3.8509316770186335e-05,
      "loss": 0.969,
      "step": 110
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.6616120338439941,
      "learning_rate": 3.788819875776398e-05,
      "loss": 0.9593,
      "step": 111
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 1.683480978012085,
      "learning_rate": 3.7267080745341614e-05,
      "loss": 0.9392,
      "step": 112
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 1.7483853101730347,
      "learning_rate": 3.6645962732919256e-05,
      "loss": 0.9756,
      "step": 113
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.170597553253174,
      "learning_rate": 3.60248447204969e-05,
      "loss": 0.9306,
      "step": 114
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 2.6813554763793945,
      "learning_rate": 3.5403726708074535e-05,
      "loss": 0.9092,
      "step": 115
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 1.9411762952804565,
      "learning_rate": 3.478260869565218e-05,
      "loss": 0.897,
      "step": 116
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 2.081491231918335,
      "learning_rate": 3.4161490683229814e-05,
      "loss": 0.9399,
      "step": 117
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 1.6819311380386353,
      "learning_rate": 3.3540372670807456e-05,
      "loss": 0.9096,
      "step": 118
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 1.6812245845794678,
      "learning_rate": 3.29192546583851e-05,
      "loss": 0.9246,
      "step": 119
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 1.8437163829803467,
      "learning_rate": 3.2298136645962735e-05,
      "loss": 0.9555,
      "step": 120
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 1.6536738872528076,
      "learning_rate": 3.167701863354037e-05,
      "loss": 0.9303,
      "step": 121
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 1.631041169166565,
      "learning_rate": 3.1055900621118014e-05,
      "loss": 0.8809,
      "step": 122
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.7272752523422241,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 0.8735,
      "step": 123
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.9515516757965088,
      "learning_rate": 2.9813664596273296e-05,
      "loss": 0.9181,
      "step": 124
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 2.3038711547851562,
      "learning_rate": 2.919254658385093e-05,
      "loss": 0.9386,
      "step": 125
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 3.035978317260742,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.9183,
      "step": 126
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 1.819288730621338,
      "learning_rate": 2.795031055900621e-05,
      "loss": 0.8869,
      "step": 127
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 1.7498723268508911,
      "learning_rate": 2.7329192546583853e-05,
      "loss": 0.9239,
      "step": 128
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.8593019247055054,
      "learning_rate": 2.6708074534161492e-05,
      "loss": 0.9241,
      "step": 129
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 1.8558043241500854,
      "learning_rate": 2.608695652173913e-05,
      "loss": 0.9016,
      "step": 130
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 1.7526891231536865,
      "learning_rate": 2.546583850931677e-05,
      "loss": 0.8878,
      "step": 131
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.8490535020828247,
      "learning_rate": 2.484472049689441e-05,
      "loss": 0.917,
      "step": 132
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 1.7780544757843018,
      "learning_rate": 2.4223602484472053e-05,
      "loss": 0.8805,
      "step": 133
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.8632535934448242,
      "learning_rate": 2.3602484472049692e-05,
      "loss": 0.9333,
      "step": 134
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 1.7859399318695068,
      "learning_rate": 2.2981366459627328e-05,
      "loss": 0.8626,
      "step": 135
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 1.750026822090149,
      "learning_rate": 2.236024844720497e-05,
      "loss": 0.9156,
      "step": 136
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 1.8439282178878784,
      "learning_rate": 2.173913043478261e-05,
      "loss": 0.9461,
      "step": 137
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 1.6232434511184692,
      "learning_rate": 2.111801242236025e-05,
      "loss": 0.8985,
      "step": 138
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.9007847309112549,
      "learning_rate": 2.049689440993789e-05,
      "loss": 0.9308,
      "step": 139
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 1.841968297958374,
      "learning_rate": 1.9875776397515528e-05,
      "loss": 0.9651,
      "step": 140
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.7510546445846558,
      "learning_rate": 1.9254658385093167e-05,
      "loss": 0.9802,
      "step": 141
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 1.9440656900405884,
      "learning_rate": 1.8633540372670807e-05,
      "loss": 0.9198,
      "step": 142
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 1.7161973714828491,
      "learning_rate": 1.801242236024845e-05,
      "loss": 0.92,
      "step": 143
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.716609001159668,
      "learning_rate": 1.739130434782609e-05,
      "loss": 0.8926,
      "step": 144
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 1.8520514965057373,
      "learning_rate": 1.6770186335403728e-05,
      "loss": 0.8805,
      "step": 145
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 1.7541126012802124,
      "learning_rate": 1.6149068322981367e-05,
      "loss": 0.8863,
      "step": 146
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 1.7308363914489746,
      "learning_rate": 1.5527950310559007e-05,
      "loss": 0.9384,
      "step": 147
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 1.7122875452041626,
      "learning_rate": 1.4906832298136648e-05,
      "loss": 0.9566,
      "step": 148
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 1.6639031171798706,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.89,
      "step": 149
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.7346582412719727,
      "learning_rate": 1.3664596273291926e-05,
      "loss": 0.9005,
      "step": 150
    },
    {
      "epoch": 2.64,
      "eval_loss": 7.238262176513672,
      "eval_runtime": 1.343,
      "eval_samples_per_second": 74.463,
      "eval_steps_per_second": 18.616,
      "step": 150
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 2.1208083629608154,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 0.9132,
      "step": 151
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 1.8750793933868408,
      "learning_rate": 1.2422360248447205e-05,
      "loss": 0.9713,
      "step": 152
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 1.7761857509613037,
      "learning_rate": 1.1801242236024846e-05,
      "loss": 0.9208,
      "step": 153
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 1.7486990690231323,
      "learning_rate": 1.1180124223602485e-05,
      "loss": 0.9649,
      "step": 154
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 1.6726531982421875,
      "learning_rate": 1.0559006211180125e-05,
      "loss": 0.9211,
      "step": 155
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 1.616739273071289,
      "learning_rate": 9.937888198757764e-06,
      "loss": 0.8998,
      "step": 156
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 1.9527230262756348,
      "learning_rate": 9.316770186335403e-06,
      "loss": 0.883,
      "step": 157
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 1.712409496307373,
      "learning_rate": 8.695652173913044e-06,
      "loss": 0.8972,
      "step": 158
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.953315258026123,
      "learning_rate": 8.074534161490684e-06,
      "loss": 0.9071,
      "step": 159
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 1.6838864088058472,
      "learning_rate": 7.453416149068324e-06,
      "loss": 0.8738,
      "step": 160
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 1.689092755317688,
      "learning_rate": 6.832298136645963e-06,
      "loss": 0.895,
      "step": 161
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 1.7176109552383423,
      "learning_rate": 6.2111801242236025e-06,
      "loss": 0.9148,
      "step": 162
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 1.7180322408676147,
      "learning_rate": 5.590062111801243e-06,
      "loss": 0.9192,
      "step": 163
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.8593027591705322,
      "learning_rate": 4.968944099378882e-06,
      "loss": 0.8811,
      "step": 164
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 1.6744641065597534,
      "learning_rate": 4.347826086956522e-06,
      "loss": 0.89,
      "step": 165
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 1.6708965301513672,
      "learning_rate": 3.726708074534162e-06,
      "loss": 0.8676,
      "step": 166
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 1.7522447109222412,
      "learning_rate": 3.1055900621118013e-06,
      "loss": 0.9387,
      "step": 167
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.6897801160812378,
      "learning_rate": 2.484472049689441e-06,
      "loss": 0.8735,
      "step": 168
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 1.6674813032150269,
      "learning_rate": 1.863354037267081e-06,
      "loss": 0.9135,
      "step": 169
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 1.7011860609054565,
      "learning_rate": 1.2422360248447205e-06,
      "loss": 0.8984,
      "step": 170
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.4426071643829346,
      "learning_rate": 6.211180124223603e-07,
      "loss": 0.9316,
      "step": 171
    }
  ],
  "logging_steps": 1,
  "max_steps": 171,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5941356738729984.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
