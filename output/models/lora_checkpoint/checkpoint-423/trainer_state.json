{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 50,
  "global_step": 423,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007104795737122558,
      "grad_norm": 9.942571640014648,
      "learning_rate": 0.0,
      "loss": 3.1396,
      "step": 1
    },
    {
      "epoch": 0.014209591474245116,
      "grad_norm": 11.972055435180664,
      "learning_rate": 1e-05,
      "loss": 3.7895,
      "step": 2
    },
    {
      "epoch": 0.021314387211367674,
      "grad_norm": 13.041999816894531,
      "learning_rate": 2e-05,
      "loss": 4.0639,
      "step": 3
    },
    {
      "epoch": 0.028419182948490232,
      "grad_norm": 11.340673446655273,
      "learning_rate": 3e-05,
      "loss": 3.6658,
      "step": 4
    },
    {
      "epoch": 0.035523978685612786,
      "grad_norm": 10.717253684997559,
      "learning_rate": 4e-05,
      "loss": 3.5118,
      "step": 5
    },
    {
      "epoch": 0.04262877442273535,
      "grad_norm": 9.628381729125977,
      "learning_rate": 5e-05,
      "loss": 3.2471,
      "step": 6
    },
    {
      "epoch": 0.0497335701598579,
      "grad_norm": 7.800248622894287,
      "learning_rate": 6e-05,
      "loss": 2.9468,
      "step": 7
    },
    {
      "epoch": 0.056838365896980464,
      "grad_norm": 7.260751724243164,
      "learning_rate": 7e-05,
      "loss": 2.9298,
      "step": 8
    },
    {
      "epoch": 0.06394316163410302,
      "grad_norm": 6.630018711090088,
      "learning_rate": 8e-05,
      "loss": 2.6951,
      "step": 9
    },
    {
      "epoch": 0.07104795737122557,
      "grad_norm": 5.354172706604004,
      "learning_rate": 9e-05,
      "loss": 2.1771,
      "step": 10
    },
    {
      "epoch": 0.07815275310834814,
      "grad_norm": 5.7045817375183105,
      "learning_rate": 0.0001,
      "loss": 2.2561,
      "step": 11
    },
    {
      "epoch": 0.0852575488454707,
      "grad_norm": 4.74889612197876,
      "learning_rate": 9.975786924939467e-05,
      "loss": 1.9838,
      "step": 12
    },
    {
      "epoch": 0.09236234458259325,
      "grad_norm": 4.7449445724487305,
      "learning_rate": 9.951573849878934e-05,
      "loss": 1.9848,
      "step": 13
    },
    {
      "epoch": 0.0994671403197158,
      "grad_norm": 4.373538017272949,
      "learning_rate": 9.927360774818402e-05,
      "loss": 1.8919,
      "step": 14
    },
    {
      "epoch": 0.10657193605683836,
      "grad_norm": 4.623711585998535,
      "learning_rate": 9.90314769975787e-05,
      "loss": 1.7985,
      "step": 15
    },
    {
      "epoch": 0.11367673179396093,
      "grad_norm": 4.627486705780029,
      "learning_rate": 9.878934624697337e-05,
      "loss": 1.7942,
      "step": 16
    },
    {
      "epoch": 0.12078152753108348,
      "grad_norm": 4.0215935707092285,
      "learning_rate": 9.854721549636804e-05,
      "loss": 1.6423,
      "step": 17
    },
    {
      "epoch": 0.12788632326820604,
      "grad_norm": 3.7291929721832275,
      "learning_rate": 9.830508474576272e-05,
      "loss": 1.58,
      "step": 18
    },
    {
      "epoch": 0.1349911190053286,
      "grad_norm": 4.068421363830566,
      "learning_rate": 9.806295399515739e-05,
      "loss": 1.5537,
      "step": 19
    },
    {
      "epoch": 0.14209591474245115,
      "grad_norm": 3.346500873565674,
      "learning_rate": 9.782082324455206e-05,
      "loss": 1.4495,
      "step": 20
    },
    {
      "epoch": 0.1492007104795737,
      "grad_norm": 3.217679262161255,
      "learning_rate": 9.757869249394673e-05,
      "loss": 1.4915,
      "step": 21
    },
    {
      "epoch": 0.15630550621669628,
      "grad_norm": 3.1423754692077637,
      "learning_rate": 9.733656174334141e-05,
      "loss": 1.3887,
      "step": 22
    },
    {
      "epoch": 0.16341030195381884,
      "grad_norm": 3.0141634941101074,
      "learning_rate": 9.709443099273609e-05,
      "loss": 1.3558,
      "step": 23
    },
    {
      "epoch": 0.1705150976909414,
      "grad_norm": 3.0585923194885254,
      "learning_rate": 9.685230024213076e-05,
      "loss": 1.3985,
      "step": 24
    },
    {
      "epoch": 0.17761989342806395,
      "grad_norm": 3.2128145694732666,
      "learning_rate": 9.661016949152543e-05,
      "loss": 1.406,
      "step": 25
    },
    {
      "epoch": 0.1847246891651865,
      "grad_norm": 2.8369851112365723,
      "learning_rate": 9.63680387409201e-05,
      "loss": 1.3229,
      "step": 26
    },
    {
      "epoch": 0.19182948490230906,
      "grad_norm": 3.2277839183807373,
      "learning_rate": 9.612590799031478e-05,
      "loss": 1.3182,
      "step": 27
    },
    {
      "epoch": 0.1989342806394316,
      "grad_norm": 3.2110137939453125,
      "learning_rate": 9.588377723970945e-05,
      "loss": 1.3748,
      "step": 28
    },
    {
      "epoch": 0.20603907637655416,
      "grad_norm": 2.585601329803467,
      "learning_rate": 9.564164648910411e-05,
      "loss": 1.2689,
      "step": 29
    },
    {
      "epoch": 0.21314387211367672,
      "grad_norm": 2.6890909671783447,
      "learning_rate": 9.53995157384988e-05,
      "loss": 1.2928,
      "step": 30
    },
    {
      "epoch": 0.2202486678507993,
      "grad_norm": 2.6011452674865723,
      "learning_rate": 9.515738498789348e-05,
      "loss": 1.2082,
      "step": 31
    },
    {
      "epoch": 0.22735346358792186,
      "grad_norm": 3.317466974258423,
      "learning_rate": 9.491525423728815e-05,
      "loss": 1.202,
      "step": 32
    },
    {
      "epoch": 0.2344582593250444,
      "grad_norm": 3.214947462081909,
      "learning_rate": 9.467312348668281e-05,
      "loss": 1.308,
      "step": 33
    },
    {
      "epoch": 0.24156305506216696,
      "grad_norm": 3.097005844116211,
      "learning_rate": 9.443099273607748e-05,
      "loss": 1.1801,
      "step": 34
    },
    {
      "epoch": 0.24866785079928952,
      "grad_norm": 2.6570651531219482,
      "learning_rate": 9.418886198547215e-05,
      "loss": 1.2206,
      "step": 35
    },
    {
      "epoch": 0.2557726465364121,
      "grad_norm": 2.942296028137207,
      "learning_rate": 9.394673123486683e-05,
      "loss": 1.2588,
      "step": 36
    },
    {
      "epoch": 0.26287744227353466,
      "grad_norm": 2.6192164421081543,
      "learning_rate": 9.370460048426151e-05,
      "loss": 1.1734,
      "step": 37
    },
    {
      "epoch": 0.2699822380106572,
      "grad_norm": 3.249988079071045,
      "learning_rate": 9.346246973365618e-05,
      "loss": 1.1778,
      "step": 38
    },
    {
      "epoch": 0.27708703374777977,
      "grad_norm": 2.522587537765503,
      "learning_rate": 9.322033898305085e-05,
      "loss": 1.1812,
      "step": 39
    },
    {
      "epoch": 0.2841918294849023,
      "grad_norm": 2.945117950439453,
      "learning_rate": 9.297820823244553e-05,
      "loss": 1.184,
      "step": 40
    },
    {
      "epoch": 0.2912966252220249,
      "grad_norm": 2.8362789154052734,
      "learning_rate": 9.27360774818402e-05,
      "loss": 1.1419,
      "step": 41
    },
    {
      "epoch": 0.2984014209591474,
      "grad_norm": 2.616267204284668,
      "learning_rate": 9.249394673123487e-05,
      "loss": 1.1688,
      "step": 42
    },
    {
      "epoch": 0.30550621669627,
      "grad_norm": 2.4010026454925537,
      "learning_rate": 9.225181598062954e-05,
      "loss": 1.1916,
      "step": 43
    },
    {
      "epoch": 0.31261101243339257,
      "grad_norm": 2.3207762241363525,
      "learning_rate": 9.200968523002422e-05,
      "loss": 1.1349,
      "step": 44
    },
    {
      "epoch": 0.3197158081705151,
      "grad_norm": 2.6678953170776367,
      "learning_rate": 9.17675544794189e-05,
      "loss": 1.1256,
      "step": 45
    },
    {
      "epoch": 0.3268206039076377,
      "grad_norm": 2.492636203765869,
      "learning_rate": 9.152542372881357e-05,
      "loss": 1.2045,
      "step": 46
    },
    {
      "epoch": 0.3339253996447602,
      "grad_norm": 2.826296806335449,
      "learning_rate": 9.128329297820824e-05,
      "loss": 1.1524,
      "step": 47
    },
    {
      "epoch": 0.3410301953818828,
      "grad_norm": 2.4356935024261475,
      "learning_rate": 9.10411622276029e-05,
      "loss": 1.1251,
      "step": 48
    },
    {
      "epoch": 0.3481349911190053,
      "grad_norm": 2.361682891845703,
      "learning_rate": 9.079903147699759e-05,
      "loss": 1.223,
      "step": 49
    },
    {
      "epoch": 0.3552397868561279,
      "grad_norm": 2.801471710205078,
      "learning_rate": 9.055690072639225e-05,
      "loss": 1.1199,
      "step": 50
    },
    {
      "epoch": 0.3552397868561279,
      "eval_loss": 5.982851028442383,
      "eval_runtime": 67.2307,
      "eval_samples_per_second": 3.719,
      "eval_steps_per_second": 0.937,
      "step": 50
    },
    {
      "epoch": 0.3623445825932504,
      "grad_norm": 2.1206979751586914,
      "learning_rate": 9.031476997578692e-05,
      "loss": 1.1439,
      "step": 51
    },
    {
      "epoch": 0.369449378330373,
      "grad_norm": 2.1084349155426025,
      "learning_rate": 9.00726392251816e-05,
      "loss": 1.055,
      "step": 52
    },
    {
      "epoch": 0.3765541740674956,
      "grad_norm": 2.0740561485290527,
      "learning_rate": 8.983050847457629e-05,
      "loss": 1.1797,
      "step": 53
    },
    {
      "epoch": 0.3836589698046181,
      "grad_norm": 2.1660056114196777,
      "learning_rate": 8.958837772397095e-05,
      "loss": 1.1422,
      "step": 54
    },
    {
      "epoch": 0.3907637655417407,
      "grad_norm": 2.7219836711883545,
      "learning_rate": 8.934624697336562e-05,
      "loss": 1.1478,
      "step": 55
    },
    {
      "epoch": 0.3978685612788632,
      "grad_norm": 2.9519007205963135,
      "learning_rate": 8.910411622276029e-05,
      "loss": 1.228,
      "step": 56
    },
    {
      "epoch": 0.4049733570159858,
      "grad_norm": 2.649702787399292,
      "learning_rate": 8.886198547215496e-05,
      "loss": 1.1653,
      "step": 57
    },
    {
      "epoch": 0.41207815275310833,
      "grad_norm": 2.3076343536376953,
      "learning_rate": 8.861985472154964e-05,
      "loss": 1.135,
      "step": 58
    },
    {
      "epoch": 0.4191829484902309,
      "grad_norm": 2.437511920928955,
      "learning_rate": 8.837772397094431e-05,
      "loss": 1.1809,
      "step": 59
    },
    {
      "epoch": 0.42628774422735344,
      "grad_norm": 2.2253637313842773,
      "learning_rate": 8.813559322033899e-05,
      "loss": 1.0696,
      "step": 60
    },
    {
      "epoch": 0.433392539964476,
      "grad_norm": 2.1373989582061768,
      "learning_rate": 8.789346246973366e-05,
      "loss": 1.0856,
      "step": 61
    },
    {
      "epoch": 0.4404973357015986,
      "grad_norm": 2.030951976776123,
      "learning_rate": 8.765133171912834e-05,
      "loss": 1.1207,
      "step": 62
    },
    {
      "epoch": 0.44760213143872113,
      "grad_norm": 2.404097557067871,
      "learning_rate": 8.740920096852301e-05,
      "loss": 1.1533,
      "step": 63
    },
    {
      "epoch": 0.4547069271758437,
      "grad_norm": 2.3249244689941406,
      "learning_rate": 8.716707021791768e-05,
      "loss": 1.1897,
      "step": 64
    },
    {
      "epoch": 0.46181172291296624,
      "grad_norm": 2.200366497039795,
      "learning_rate": 8.692493946731234e-05,
      "loss": 1.115,
      "step": 65
    },
    {
      "epoch": 0.4689165186500888,
      "grad_norm": 2.0929908752441406,
      "learning_rate": 8.668280871670703e-05,
      "loss": 1.1408,
      "step": 66
    },
    {
      "epoch": 0.47602131438721135,
      "grad_norm": 2.157815456390381,
      "learning_rate": 8.644067796610171e-05,
      "loss": 1.0939,
      "step": 67
    },
    {
      "epoch": 0.48312611012433393,
      "grad_norm": 2.1124887466430664,
      "learning_rate": 8.619854721549638e-05,
      "loss": 1.144,
      "step": 68
    },
    {
      "epoch": 0.49023090586145646,
      "grad_norm": 2.7435169219970703,
      "learning_rate": 8.595641646489104e-05,
      "loss": 1.1705,
      "step": 69
    },
    {
      "epoch": 0.49733570159857904,
      "grad_norm": 2.2719147205352783,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.1538,
      "step": 70
    },
    {
      "epoch": 0.5044404973357016,
      "grad_norm": 2.1503114700317383,
      "learning_rate": 8.54721549636804e-05,
      "loss": 1.0508,
      "step": 71
    },
    {
      "epoch": 0.5115452930728241,
      "grad_norm": 1.809924602508545,
      "learning_rate": 8.523002421307506e-05,
      "loss": 1.1016,
      "step": 72
    },
    {
      "epoch": 0.5186500888099467,
      "grad_norm": 2.366171360015869,
      "learning_rate": 8.498789346246973e-05,
      "loss": 1.1684,
      "step": 73
    },
    {
      "epoch": 0.5257548845470693,
      "grad_norm": 2.3184287548065186,
      "learning_rate": 8.474576271186441e-05,
      "loss": 1.1911,
      "step": 74
    },
    {
      "epoch": 0.5328596802841918,
      "grad_norm": 2.147047281265259,
      "learning_rate": 8.45036319612591e-05,
      "loss": 1.2057,
      "step": 75
    },
    {
      "epoch": 0.5399644760213144,
      "grad_norm": 2.291131019592285,
      "learning_rate": 8.426150121065376e-05,
      "loss": 1.1527,
      "step": 76
    },
    {
      "epoch": 0.5470692717584369,
      "grad_norm": 2.106499433517456,
      "learning_rate": 8.401937046004843e-05,
      "loss": 1.138,
      "step": 77
    },
    {
      "epoch": 0.5541740674955595,
      "grad_norm": 2.044391632080078,
      "learning_rate": 8.37772397094431e-05,
      "loss": 1.0281,
      "step": 78
    },
    {
      "epoch": 0.5612788632326821,
      "grad_norm": 1.728295922279358,
      "learning_rate": 8.353510895883777e-05,
      "loss": 1.0836,
      "step": 79
    },
    {
      "epoch": 0.5683836589698046,
      "grad_norm": 2.071669578552246,
      "learning_rate": 8.329297820823245e-05,
      "loss": 1.2141,
      "step": 80
    },
    {
      "epoch": 0.5754884547069272,
      "grad_norm": 1.965598464012146,
      "learning_rate": 8.305084745762712e-05,
      "loss": 1.1,
      "step": 81
    },
    {
      "epoch": 0.5825932504440497,
      "grad_norm": 1.7369966506958008,
      "learning_rate": 8.28087167070218e-05,
      "loss": 1.1613,
      "step": 82
    },
    {
      "epoch": 0.5896980461811723,
      "grad_norm": 2.034379482269287,
      "learning_rate": 8.256658595641647e-05,
      "loss": 1.1095,
      "step": 83
    },
    {
      "epoch": 0.5968028419182948,
      "grad_norm": 2.06583309173584,
      "learning_rate": 8.232445520581115e-05,
      "loss": 1.102,
      "step": 84
    },
    {
      "epoch": 0.6039076376554174,
      "grad_norm": 2.0921757221221924,
      "learning_rate": 8.208232445520582e-05,
      "loss": 1.1498,
      "step": 85
    },
    {
      "epoch": 0.61101243339254,
      "grad_norm": 1.8711073398590088,
      "learning_rate": 8.184019370460048e-05,
      "loss": 1.0277,
      "step": 86
    },
    {
      "epoch": 0.6181172291296625,
      "grad_norm": 1.976163387298584,
      "learning_rate": 8.159806295399515e-05,
      "loss": 1.2085,
      "step": 87
    },
    {
      "epoch": 0.6252220248667851,
      "grad_norm": 2.154554605484009,
      "learning_rate": 8.135593220338983e-05,
      "loss": 1.116,
      "step": 88
    },
    {
      "epoch": 0.6323268206039077,
      "grad_norm": 1.9763095378875732,
      "learning_rate": 8.111380145278452e-05,
      "loss": 1.0595,
      "step": 89
    },
    {
      "epoch": 0.6394316163410302,
      "grad_norm": 1.933020830154419,
      "learning_rate": 8.087167070217918e-05,
      "loss": 1.126,
      "step": 90
    },
    {
      "epoch": 0.6465364120781527,
      "grad_norm": 1.9036865234375,
      "learning_rate": 8.062953995157385e-05,
      "loss": 1.1348,
      "step": 91
    },
    {
      "epoch": 0.6536412078152753,
      "grad_norm": 1.7192600965499878,
      "learning_rate": 8.038740920096852e-05,
      "loss": 1.1257,
      "step": 92
    },
    {
      "epoch": 0.6607460035523979,
      "grad_norm": 1.9741352796554565,
      "learning_rate": 8.01452784503632e-05,
      "loss": 1.2017,
      "step": 93
    },
    {
      "epoch": 0.6678507992895204,
      "grad_norm": 1.8359029293060303,
      "learning_rate": 7.990314769975787e-05,
      "loss": 1.0602,
      "step": 94
    },
    {
      "epoch": 0.6749555950266429,
      "grad_norm": 1.7180720567703247,
      "learning_rate": 7.966101694915254e-05,
      "loss": 1.0725,
      "step": 95
    },
    {
      "epoch": 0.6820603907637656,
      "grad_norm": 1.7822092771530151,
      "learning_rate": 7.941888619854722e-05,
      "loss": 1.0175,
      "step": 96
    },
    {
      "epoch": 0.6891651865008881,
      "grad_norm": 1.9274643659591675,
      "learning_rate": 7.91767554479419e-05,
      "loss": 1.0764,
      "step": 97
    },
    {
      "epoch": 0.6962699822380106,
      "grad_norm": 1.6523760557174683,
      "learning_rate": 7.893462469733657e-05,
      "loss": 1.0364,
      "step": 98
    },
    {
      "epoch": 0.7033747779751333,
      "grad_norm": 2.004192590713501,
      "learning_rate": 7.869249394673124e-05,
      "loss": 1.0175,
      "step": 99
    },
    {
      "epoch": 0.7104795737122558,
      "grad_norm": 1.8760229349136353,
      "learning_rate": 7.845036319612591e-05,
      "loss": 1.1066,
      "step": 100
    },
    {
      "epoch": 0.7104795737122558,
      "eval_loss": 5.942786693572998,
      "eval_runtime": 4.4292,
      "eval_samples_per_second": 56.444,
      "eval_steps_per_second": 14.224,
      "step": 100
    },
    {
      "epoch": 0.7175843694493783,
      "grad_norm": 2.1369245052337646,
      "learning_rate": 7.820823244552058e-05,
      "loss": 1.0824,
      "step": 101
    },
    {
      "epoch": 0.7246891651865008,
      "grad_norm": 2.046448230743408,
      "learning_rate": 7.796610169491526e-05,
      "loss": 1.0781,
      "step": 102
    },
    {
      "epoch": 0.7317939609236235,
      "grad_norm": 1.9243519306182861,
      "learning_rate": 7.772397094430993e-05,
      "loss": 1.1245,
      "step": 103
    },
    {
      "epoch": 0.738898756660746,
      "grad_norm": 2.384803533554077,
      "learning_rate": 7.748184019370461e-05,
      "loss": 1.1176,
      "step": 104
    },
    {
      "epoch": 0.7460035523978685,
      "grad_norm": 1.898044466972351,
      "learning_rate": 7.723970944309928e-05,
      "loss": 1.143,
      "step": 105
    },
    {
      "epoch": 0.7531083481349912,
      "grad_norm": 1.7047823667526245,
      "learning_rate": 7.699757869249396e-05,
      "loss": 1.0887,
      "step": 106
    },
    {
      "epoch": 0.7602131438721137,
      "grad_norm": 2.1180341243743896,
      "learning_rate": 7.675544794188863e-05,
      "loss": 0.9964,
      "step": 107
    },
    {
      "epoch": 0.7673179396092362,
      "grad_norm": 1.7994502782821655,
      "learning_rate": 7.65133171912833e-05,
      "loss": 1.1068,
      "step": 108
    },
    {
      "epoch": 0.7744227353463587,
      "grad_norm": 1.7763957977294922,
      "learning_rate": 7.627118644067796e-05,
      "loss": 1.1461,
      "step": 109
    },
    {
      "epoch": 0.7815275310834814,
      "grad_norm": 1.9147433042526245,
      "learning_rate": 7.602905569007264e-05,
      "loss": 1.0372,
      "step": 110
    },
    {
      "epoch": 0.7886323268206039,
      "grad_norm": 2.0463292598724365,
      "learning_rate": 7.578692493946733e-05,
      "loss": 1.1151,
      "step": 111
    },
    {
      "epoch": 0.7957371225577264,
      "grad_norm": 1.7321972846984863,
      "learning_rate": 7.5544794188862e-05,
      "loss": 1.0578,
      "step": 112
    },
    {
      "epoch": 0.8028419182948491,
      "grad_norm": 1.9640741348266602,
      "learning_rate": 7.530266343825666e-05,
      "loss": 1.104,
      "step": 113
    },
    {
      "epoch": 0.8099467140319716,
      "grad_norm": 1.825929880142212,
      "learning_rate": 7.506053268765133e-05,
      "loss": 1.0412,
      "step": 114
    },
    {
      "epoch": 0.8170515097690941,
      "grad_norm": 1.7147064208984375,
      "learning_rate": 7.481840193704601e-05,
      "loss": 1.0183,
      "step": 115
    },
    {
      "epoch": 0.8241563055062167,
      "grad_norm": 2.2576329708099365,
      "learning_rate": 7.457627118644068e-05,
      "loss": 1.0173,
      "step": 116
    },
    {
      "epoch": 0.8312611012433393,
      "grad_norm": 1.9825193881988525,
      "learning_rate": 7.433414043583535e-05,
      "loss": 1.098,
      "step": 117
    },
    {
      "epoch": 0.8383658969804618,
      "grad_norm": 2.041274309158325,
      "learning_rate": 7.409200968523003e-05,
      "loss": 1.1599,
      "step": 118
    },
    {
      "epoch": 0.8454706927175843,
      "grad_norm": 1.8379871845245361,
      "learning_rate": 7.384987893462471e-05,
      "loss": 1.0463,
      "step": 119
    },
    {
      "epoch": 0.8525754884547069,
      "grad_norm": 1.760847568511963,
      "learning_rate": 7.360774818401938e-05,
      "loss": 1.0617,
      "step": 120
    },
    {
      "epoch": 0.8596802841918295,
      "grad_norm": 2.077657699584961,
      "learning_rate": 7.336561743341405e-05,
      "loss": 1.016,
      "step": 121
    },
    {
      "epoch": 0.866785079928952,
      "grad_norm": 2.376711130142212,
      "learning_rate": 7.312348668280872e-05,
      "loss": 1.0895,
      "step": 122
    },
    {
      "epoch": 0.8738898756660746,
      "grad_norm": 2.1202211380004883,
      "learning_rate": 7.288135593220338e-05,
      "loss": 1.145,
      "step": 123
    },
    {
      "epoch": 0.8809946714031972,
      "grad_norm": 2.2225704193115234,
      "learning_rate": 7.263922518159807e-05,
      "loss": 1.0141,
      "step": 124
    },
    {
      "epoch": 0.8880994671403197,
      "grad_norm": 1.8006750345230103,
      "learning_rate": 7.239709443099273e-05,
      "loss": 1.0776,
      "step": 125
    },
    {
      "epoch": 0.8952042628774423,
      "grad_norm": 2.3447771072387695,
      "learning_rate": 7.215496368038742e-05,
      "loss": 1.1065,
      "step": 126
    },
    {
      "epoch": 0.9023090586145648,
      "grad_norm": 2.0760438442230225,
      "learning_rate": 7.191283292978208e-05,
      "loss": 1.0984,
      "step": 127
    },
    {
      "epoch": 0.9094138543516874,
      "grad_norm": 2.1264829635620117,
      "learning_rate": 7.167070217917677e-05,
      "loss": 1.0825,
      "step": 128
    },
    {
      "epoch": 0.91651865008881,
      "grad_norm": 2.020350217819214,
      "learning_rate": 7.142857142857143e-05,
      "loss": 1.0482,
      "step": 129
    },
    {
      "epoch": 0.9236234458259325,
      "grad_norm": 1.9845556020736694,
      "learning_rate": 7.11864406779661e-05,
      "loss": 1.0346,
      "step": 130
    },
    {
      "epoch": 0.9307282415630551,
      "grad_norm": 1.885338544845581,
      "learning_rate": 7.094430992736077e-05,
      "loss": 1.0939,
      "step": 131
    },
    {
      "epoch": 0.9378330373001776,
      "grad_norm": 1.7823046445846558,
      "learning_rate": 7.070217917675545e-05,
      "loss": 1.1427,
      "step": 132
    },
    {
      "epoch": 0.9449378330373002,
      "grad_norm": 1.8958510160446167,
      "learning_rate": 7.046004842615013e-05,
      "loss": 1.0908,
      "step": 133
    },
    {
      "epoch": 0.9520426287744227,
      "grad_norm": 1.758818507194519,
      "learning_rate": 7.02179176755448e-05,
      "loss": 1.0872,
      "step": 134
    },
    {
      "epoch": 0.9591474245115453,
      "grad_norm": 1.8980406522750854,
      "learning_rate": 6.997578692493947e-05,
      "loss": 1.0447,
      "step": 135
    },
    {
      "epoch": 0.9662522202486679,
      "grad_norm": 1.918329119682312,
      "learning_rate": 6.973365617433414e-05,
      "loss": 1.0396,
      "step": 136
    },
    {
      "epoch": 0.9733570159857904,
      "grad_norm": 1.9033105373382568,
      "learning_rate": 6.949152542372882e-05,
      "loss": 1.1654,
      "step": 137
    },
    {
      "epoch": 0.9804618117229129,
      "grad_norm": 1.7377535104751587,
      "learning_rate": 6.924939467312349e-05,
      "loss": 1.0191,
      "step": 138
    },
    {
      "epoch": 0.9875666074600356,
      "grad_norm": 1.8489010334014893,
      "learning_rate": 6.900726392251816e-05,
      "loss": 1.0964,
      "step": 139
    },
    {
      "epoch": 0.9946714031971581,
      "grad_norm": 1.7425934076309204,
      "learning_rate": 6.876513317191284e-05,
      "loss": 1.0453,
      "step": 140
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.422858238220215,
      "learning_rate": 6.852300242130752e-05,
      "loss": 1.0453,
      "step": 141
    },
    {
      "epoch": 1.0071047957371226,
      "grad_norm": 1.8025951385498047,
      "learning_rate": 6.828087167070219e-05,
      "loss": 1.0079,
      "step": 142
    },
    {
      "epoch": 1.014209591474245,
      "grad_norm": 1.8359013795852661,
      "learning_rate": 6.803874092009686e-05,
      "loss": 1.0412,
      "step": 143
    },
    {
      "epoch": 1.0213143872113677,
      "grad_norm": 2.03707218170166,
      "learning_rate": 6.779661016949152e-05,
      "loss": 1.0756,
      "step": 144
    },
    {
      "epoch": 1.0284191829484903,
      "grad_norm": 1.9077945947647095,
      "learning_rate": 6.755447941888619e-05,
      "loss": 1.0117,
      "step": 145
    },
    {
      "epoch": 1.0355239786856127,
      "grad_norm": 1.8806673288345337,
      "learning_rate": 6.731234866828087e-05,
      "loss": 1.0672,
      "step": 146
    },
    {
      "epoch": 1.0426287744227354,
      "grad_norm": 1.804182767868042,
      "learning_rate": 6.707021791767554e-05,
      "loss": 1.0607,
      "step": 147
    },
    {
      "epoch": 1.0497335701598578,
      "grad_norm": 2.1935691833496094,
      "learning_rate": 6.682808716707022e-05,
      "loss": 1.0505,
      "step": 148
    },
    {
      "epoch": 1.0568383658969804,
      "grad_norm": 1.8698712587356567,
      "learning_rate": 6.658595641646489e-05,
      "loss": 0.9827,
      "step": 149
    },
    {
      "epoch": 1.063943161634103,
      "grad_norm": 1.7678828239440918,
      "learning_rate": 6.634382566585957e-05,
      "loss": 1.0061,
      "step": 150
    },
    {
      "epoch": 1.063943161634103,
      "eval_loss": 5.914656162261963,
      "eval_runtime": 3.8836,
      "eval_samples_per_second": 64.373,
      "eval_steps_per_second": 16.222,
      "step": 150
    },
    {
      "epoch": 1.0710479573712255,
      "grad_norm": 1.5656691789627075,
      "learning_rate": 6.610169491525424e-05,
      "loss": 1.0468,
      "step": 151
    },
    {
      "epoch": 1.0781527531083481,
      "grad_norm": 2.205775260925293,
      "learning_rate": 6.585956416464891e-05,
      "loss": 1.1112,
      "step": 152
    },
    {
      "epoch": 1.0852575488454708,
      "grad_norm": 1.8450939655303955,
      "learning_rate": 6.561743341404358e-05,
      "loss": 0.9787,
      "step": 153
    },
    {
      "epoch": 1.0923623445825932,
      "grad_norm": 1.9915273189544678,
      "learning_rate": 6.537530266343826e-05,
      "loss": 0.9885,
      "step": 154
    },
    {
      "epoch": 1.0994671403197158,
      "grad_norm": 2.0589792728424072,
      "learning_rate": 6.513317191283293e-05,
      "loss": 1.0308,
      "step": 155
    },
    {
      "epoch": 1.1065719360568385,
      "grad_norm": 1.9618546962738037,
      "learning_rate": 6.489104116222761e-05,
      "loss": 1.0204,
      "step": 156
    },
    {
      "epoch": 1.1136767317939609,
      "grad_norm": 2.41434645652771,
      "learning_rate": 6.464891041162228e-05,
      "loss": 1.0206,
      "step": 157
    },
    {
      "epoch": 1.1207815275310835,
      "grad_norm": 2.1386654376983643,
      "learning_rate": 6.440677966101695e-05,
      "loss": 1.0633,
      "step": 158
    },
    {
      "epoch": 1.1278863232682061,
      "grad_norm": 1.851291537284851,
      "learning_rate": 6.416464891041163e-05,
      "loss": 1.0293,
      "step": 159
    },
    {
      "epoch": 1.1349911190053286,
      "grad_norm": 2.0009326934814453,
      "learning_rate": 6.39225181598063e-05,
      "loss": 1.0398,
      "step": 160
    },
    {
      "epoch": 1.1420959147424512,
      "grad_norm": 3.0895845890045166,
      "learning_rate": 6.368038740920097e-05,
      "loss": 1.0185,
      "step": 161
    },
    {
      "epoch": 1.1492007104795736,
      "grad_norm": 1.9644101858139038,
      "learning_rate": 6.343825665859565e-05,
      "loss": 1.0846,
      "step": 162
    },
    {
      "epoch": 1.1563055062166963,
      "grad_norm": 2.017430543899536,
      "learning_rate": 6.319612590799033e-05,
      "loss": 0.9764,
      "step": 163
    },
    {
      "epoch": 1.163410301953819,
      "grad_norm": 2.008842945098877,
      "learning_rate": 6.2953995157385e-05,
      "loss": 1.0322,
      "step": 164
    },
    {
      "epoch": 1.1705150976909413,
      "grad_norm": 2.094529867172241,
      "learning_rate": 6.271186440677966e-05,
      "loss": 1.0633,
      "step": 165
    },
    {
      "epoch": 1.177619893428064,
      "grad_norm": 2.3621878623962402,
      "learning_rate": 6.246973365617433e-05,
      "loss": 1.1309,
      "step": 166
    },
    {
      "epoch": 1.1847246891651866,
      "grad_norm": 1.952095866203308,
      "learning_rate": 6.222760290556901e-05,
      "loss": 1.0112,
      "step": 167
    },
    {
      "epoch": 1.191829484902309,
      "grad_norm": 1.8299517631530762,
      "learning_rate": 6.198547215496368e-05,
      "loss": 1.0541,
      "step": 168
    },
    {
      "epoch": 1.1989342806394316,
      "grad_norm": 1.710349202156067,
      "learning_rate": 6.174334140435835e-05,
      "loss": 1.0198,
      "step": 169
    },
    {
      "epoch": 1.206039076376554,
      "grad_norm": 1.9138176441192627,
      "learning_rate": 6.150121065375303e-05,
      "loss": 1.0207,
      "step": 170
    },
    {
      "epoch": 1.2131438721136767,
      "grad_norm": 1.981111764907837,
      "learning_rate": 6.12590799031477e-05,
      "loss": 0.9671,
      "step": 171
    },
    {
      "epoch": 1.2202486678507993,
      "grad_norm": 1.9057419300079346,
      "learning_rate": 6.101694915254238e-05,
      "loss": 0.9596,
      "step": 172
    },
    {
      "epoch": 1.2273534635879217,
      "grad_norm": 2.0325989723205566,
      "learning_rate": 6.077481840193705e-05,
      "loss": 1.0581,
      "step": 173
    },
    {
      "epoch": 1.2344582593250444,
      "grad_norm": 2.3017404079437256,
      "learning_rate": 6.053268765133172e-05,
      "loss": 0.9878,
      "step": 174
    },
    {
      "epoch": 1.241563055062167,
      "grad_norm": 2.251324415206909,
      "learning_rate": 6.0290556900726394e-05,
      "loss": 1.0938,
      "step": 175
    },
    {
      "epoch": 1.2486678507992894,
      "grad_norm": 2.003356695175171,
      "learning_rate": 6.0048426150121076e-05,
      "loss": 0.9751,
      "step": 176
    },
    {
      "epoch": 1.255772646536412,
      "grad_norm": 2.022334337234497,
      "learning_rate": 5.9806295399515744e-05,
      "loss": 0.9812,
      "step": 177
    },
    {
      "epoch": 1.2628774422735347,
      "grad_norm": 2.264556407928467,
      "learning_rate": 5.956416464891041e-05,
      "loss": 0.9843,
      "step": 178
    },
    {
      "epoch": 1.2699822380106571,
      "grad_norm": 2.172750234603882,
      "learning_rate": 5.932203389830509e-05,
      "loss": 0.9916,
      "step": 179
    },
    {
      "epoch": 1.2770870337477798,
      "grad_norm": 2.1967947483062744,
      "learning_rate": 5.9079903147699756e-05,
      "loss": 1.0188,
      "step": 180
    },
    {
      "epoch": 1.2841918294849024,
      "grad_norm": 2.199690580368042,
      "learning_rate": 5.883777239709444e-05,
      "loss": 1.0372,
      "step": 181
    },
    {
      "epoch": 1.2912966252220248,
      "grad_norm": 2.04250168800354,
      "learning_rate": 5.8595641646489105e-05,
      "loss": 1.0302,
      "step": 182
    },
    {
      "epoch": 1.2984014209591475,
      "grad_norm": 2.014718770980835,
      "learning_rate": 5.835351089588378e-05,
      "loss": 1.0236,
      "step": 183
    },
    {
      "epoch": 1.30550621669627,
      "grad_norm": 2.192448139190674,
      "learning_rate": 5.811138014527845e-05,
      "loss": 1.0692,
      "step": 184
    },
    {
      "epoch": 1.3126110124333925,
      "grad_norm": 2.183330774307251,
      "learning_rate": 5.786924939467313e-05,
      "loss": 1.026,
      "step": 185
    },
    {
      "epoch": 1.3197158081705151,
      "grad_norm": 2.326831102371216,
      "learning_rate": 5.76271186440678e-05,
      "loss": 1.0151,
      "step": 186
    },
    {
      "epoch": 1.3268206039076378,
      "grad_norm": 1.9532355070114136,
      "learning_rate": 5.7384987893462474e-05,
      "loss": 0.9613,
      "step": 187
    },
    {
      "epoch": 1.3339253996447602,
      "grad_norm": 1.6965951919555664,
      "learning_rate": 5.714285714285714e-05,
      "loss": 1.0983,
      "step": 188
    },
    {
      "epoch": 1.3410301953818828,
      "grad_norm": 1.6800873279571533,
      "learning_rate": 5.6900726392251823e-05,
      "loss": 1.0603,
      "step": 189
    },
    {
      "epoch": 1.3481349911190053,
      "grad_norm": 1.8231104612350464,
      "learning_rate": 5.665859564164649e-05,
      "loss": 1.0534,
      "step": 190
    },
    {
      "epoch": 1.355239786856128,
      "grad_norm": 1.9929161071777344,
      "learning_rate": 5.641646489104117e-05,
      "loss": 1.1109,
      "step": 191
    },
    {
      "epoch": 1.3623445825932503,
      "grad_norm": 2.1907362937927246,
      "learning_rate": 5.6174334140435835e-05,
      "loss": 1.0019,
      "step": 192
    },
    {
      "epoch": 1.369449378330373,
      "grad_norm": 1.8945202827453613,
      "learning_rate": 5.593220338983051e-05,
      "loss": 0.9934,
      "step": 193
    },
    {
      "epoch": 1.3765541740674956,
      "grad_norm": 2.022387742996216,
      "learning_rate": 5.569007263922519e-05,
      "loss": 1.0401,
      "step": 194
    },
    {
      "epoch": 1.383658969804618,
      "grad_norm": 1.8226032257080078,
      "learning_rate": 5.544794188861986e-05,
      "loss": 1.0775,
      "step": 195
    },
    {
      "epoch": 1.3907637655417406,
      "grad_norm": 2.115532636642456,
      "learning_rate": 5.520581113801453e-05,
      "loss": 1.041,
      "step": 196
    },
    {
      "epoch": 1.3978685612788633,
      "grad_norm": 2.0216481685638428,
      "learning_rate": 5.49636803874092e-05,
      "loss": 1.0281,
      "step": 197
    },
    {
      "epoch": 1.4049733570159857,
      "grad_norm": 2.247814655303955,
      "learning_rate": 5.4721549636803885e-05,
      "loss": 1.0779,
      "step": 198
    },
    {
      "epoch": 1.4120781527531083,
      "grad_norm": 2.0836524963378906,
      "learning_rate": 5.447941888619855e-05,
      "loss": 1.0534,
      "step": 199
    },
    {
      "epoch": 1.419182948490231,
      "grad_norm": 2.226940393447876,
      "learning_rate": 5.423728813559322e-05,
      "loss": 1.0512,
      "step": 200
    },
    {
      "epoch": 1.419182948490231,
      "eval_loss": 5.922358989715576,
      "eval_runtime": 4.0354,
      "eval_samples_per_second": 61.952,
      "eval_steps_per_second": 15.612,
      "step": 200
    },
    {
      "epoch": 1.4262877442273534,
      "grad_norm": 2.195709705352783,
      "learning_rate": 5.3995157384987896e-05,
      "loss": 0.9624,
      "step": 201
    },
    {
      "epoch": 1.433392539964476,
      "grad_norm": 1.818159580230713,
      "learning_rate": 5.3753026634382564e-05,
      "loss": 1.08,
      "step": 202
    },
    {
      "epoch": 1.4404973357015987,
      "grad_norm": 1.8615331649780273,
      "learning_rate": 5.3510895883777246e-05,
      "loss": 0.9599,
      "step": 203
    },
    {
      "epoch": 1.447602131438721,
      "grad_norm": 2.0357983112335205,
      "learning_rate": 5.3268765133171914e-05,
      "loss": 1.0743,
      "step": 204
    },
    {
      "epoch": 1.4547069271758437,
      "grad_norm": 1.918842077255249,
      "learning_rate": 5.302663438256659e-05,
      "loss": 1.073,
      "step": 205
    },
    {
      "epoch": 1.4618117229129663,
      "grad_norm": 1.8524590730667114,
      "learning_rate": 5.278450363196126e-05,
      "loss": 0.9955,
      "step": 206
    },
    {
      "epoch": 1.4689165186500888,
      "grad_norm": 2.137296199798584,
      "learning_rate": 5.254237288135594e-05,
      "loss": 1.0241,
      "step": 207
    },
    {
      "epoch": 1.4760213143872114,
      "grad_norm": 1.886653184890747,
      "learning_rate": 5.230024213075061e-05,
      "loss": 1.0369,
      "step": 208
    },
    {
      "epoch": 1.483126110124334,
      "grad_norm": 2.46313738822937,
      "learning_rate": 5.205811138014528e-05,
      "loss": 1.0047,
      "step": 209
    },
    {
      "epoch": 1.4902309058614565,
      "grad_norm": 2.1078040599823,
      "learning_rate": 5.181598062953995e-05,
      "loss": 1.0224,
      "step": 210
    },
    {
      "epoch": 1.497335701598579,
      "grad_norm": 2.2958827018737793,
      "learning_rate": 5.157384987893463e-05,
      "loss": 1.0189,
      "step": 211
    },
    {
      "epoch": 1.5044404973357017,
      "grad_norm": 1.9310293197631836,
      "learning_rate": 5.13317191283293e-05,
      "loss": 1.0248,
      "step": 212
    },
    {
      "epoch": 1.5115452930728241,
      "grad_norm": 2.3447365760803223,
      "learning_rate": 5.1089588377723975e-05,
      "loss": 0.9677,
      "step": 213
    },
    {
      "epoch": 1.5186500888099466,
      "grad_norm": 1.9686495065689087,
      "learning_rate": 5.0847457627118643e-05,
      "loss": 1.0288,
      "step": 214
    },
    {
      "epoch": 1.5257548845470694,
      "grad_norm": 2.18595814704895,
      "learning_rate": 5.060532687651331e-05,
      "loss": 1.0088,
      "step": 215
    },
    {
      "epoch": 1.5328596802841918,
      "grad_norm": 1.974363088607788,
      "learning_rate": 5.036319612590799e-05,
      "loss": 1.0707,
      "step": 216
    },
    {
      "epoch": 1.5399644760213143,
      "grad_norm": 2.0020010471343994,
      "learning_rate": 5.012106537530267e-05,
      "loss": 0.9745,
      "step": 217
    },
    {
      "epoch": 1.547069271758437,
      "grad_norm": 1.9576220512390137,
      "learning_rate": 4.9878934624697336e-05,
      "loss": 1.0161,
      "step": 218
    },
    {
      "epoch": 1.5541740674955595,
      "grad_norm": 1.915252923965454,
      "learning_rate": 4.963680387409201e-05,
      "loss": 1.0899,
      "step": 219
    },
    {
      "epoch": 1.561278863232682,
      "grad_norm": 2.1460318565368652,
      "learning_rate": 4.9394673123486686e-05,
      "loss": 1.0708,
      "step": 220
    },
    {
      "epoch": 1.5683836589698046,
      "grad_norm": 2.004056692123413,
      "learning_rate": 4.915254237288136e-05,
      "loss": 1.0841,
      "step": 221
    },
    {
      "epoch": 1.5754884547069272,
      "grad_norm": 1.9618778228759766,
      "learning_rate": 4.891041162227603e-05,
      "loss": 1.0622,
      "step": 222
    },
    {
      "epoch": 1.5825932504440496,
      "grad_norm": 1.8121095895767212,
      "learning_rate": 4.8668280871670705e-05,
      "loss": 1.0519,
      "step": 223
    },
    {
      "epoch": 1.5896980461811723,
      "grad_norm": 1.867557168006897,
      "learning_rate": 4.842615012106538e-05,
      "loss": 1.0427,
      "step": 224
    },
    {
      "epoch": 1.596802841918295,
      "grad_norm": 1.7362456321716309,
      "learning_rate": 4.818401937046005e-05,
      "loss": 1.068,
      "step": 225
    },
    {
      "epoch": 1.6039076376554173,
      "grad_norm": 1.8068761825561523,
      "learning_rate": 4.794188861985472e-05,
      "loss": 1.1162,
      "step": 226
    },
    {
      "epoch": 1.61101243339254,
      "grad_norm": 1.8314632177352905,
      "learning_rate": 4.76997578692494e-05,
      "loss": 0.9825,
      "step": 227
    },
    {
      "epoch": 1.6181172291296626,
      "grad_norm": 1.9930996894836426,
      "learning_rate": 4.745762711864407e-05,
      "loss": 1.0299,
      "step": 228
    },
    {
      "epoch": 1.625222024866785,
      "grad_norm": 1.68857741355896,
      "learning_rate": 4.721549636803874e-05,
      "loss": 1.0574,
      "step": 229
    },
    {
      "epoch": 1.6323268206039077,
      "grad_norm": 2.3031227588653564,
      "learning_rate": 4.6973365617433416e-05,
      "loss": 1.0637,
      "step": 230
    },
    {
      "epoch": 1.6394316163410303,
      "grad_norm": 1.9545453786849976,
      "learning_rate": 4.673123486682809e-05,
      "loss": 1.0504,
      "step": 231
    },
    {
      "epoch": 1.6465364120781527,
      "grad_norm": 2.2046992778778076,
      "learning_rate": 4.6489104116222766e-05,
      "loss": 1.0394,
      "step": 232
    },
    {
      "epoch": 1.6536412078152753,
      "grad_norm": 1.8462104797363281,
      "learning_rate": 4.6246973365617434e-05,
      "loss": 1.0036,
      "step": 233
    },
    {
      "epoch": 1.660746003552398,
      "grad_norm": 1.983008861541748,
      "learning_rate": 4.600484261501211e-05,
      "loss": 1.0631,
      "step": 234
    },
    {
      "epoch": 1.6678507992895204,
      "grad_norm": 1.789856195449829,
      "learning_rate": 4.5762711864406784e-05,
      "loss": 1.0409,
      "step": 235
    },
    {
      "epoch": 1.6749555950266428,
      "grad_norm": 1.8267685174942017,
      "learning_rate": 4.552058111380145e-05,
      "loss": 1.0688,
      "step": 236
    },
    {
      "epoch": 1.6820603907637657,
      "grad_norm": 1.900468111038208,
      "learning_rate": 4.527845036319613e-05,
      "loss": 1.0331,
      "step": 237
    },
    {
      "epoch": 1.689165186500888,
      "grad_norm": 1.765519142150879,
      "learning_rate": 4.50363196125908e-05,
      "loss": 1.0258,
      "step": 238
    },
    {
      "epoch": 1.6962699822380105,
      "grad_norm": 2.6924545764923096,
      "learning_rate": 4.479418886198548e-05,
      "loss": 1.0385,
      "step": 239
    },
    {
      "epoch": 1.7033747779751334,
      "grad_norm": 2.1623620986938477,
      "learning_rate": 4.4552058111380145e-05,
      "loss": 0.9652,
      "step": 240
    },
    {
      "epoch": 1.7104795737122558,
      "grad_norm": 1.7733720541000366,
      "learning_rate": 4.430992736077482e-05,
      "loss": 1.0241,
      "step": 241
    },
    {
      "epoch": 1.7175843694493782,
      "grad_norm": 1.7495861053466797,
      "learning_rate": 4.4067796610169495e-05,
      "loss": 1.0004,
      "step": 242
    },
    {
      "epoch": 1.7246891651865008,
      "grad_norm": 1.673025131225586,
      "learning_rate": 4.382566585956417e-05,
      "loss": 1.0023,
      "step": 243
    },
    {
      "epoch": 1.7317939609236235,
      "grad_norm": 2.0711944103240967,
      "learning_rate": 4.358353510895884e-05,
      "loss": 0.9728,
      "step": 244
    },
    {
      "epoch": 1.738898756660746,
      "grad_norm": 2.016657829284668,
      "learning_rate": 4.334140435835351e-05,
      "loss": 0.9217,
      "step": 245
    },
    {
      "epoch": 1.7460035523978685,
      "grad_norm": 1.6989431381225586,
      "learning_rate": 4.309927360774819e-05,
      "loss": 1.0284,
      "step": 246
    },
    {
      "epoch": 1.7531083481349912,
      "grad_norm": 2.0455591678619385,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 1.0302,
      "step": 247
    },
    {
      "epoch": 1.7602131438721136,
      "grad_norm": 1.8547298908233643,
      "learning_rate": 4.261501210653753e-05,
      "loss": 0.9965,
      "step": 248
    },
    {
      "epoch": 1.7673179396092362,
      "grad_norm": 2.2342376708984375,
      "learning_rate": 4.2372881355932206e-05,
      "loss": 0.9879,
      "step": 249
    },
    {
      "epoch": 1.7744227353463589,
      "grad_norm": 1.6745294332504272,
      "learning_rate": 4.213075060532688e-05,
      "loss": 1.0047,
      "step": 250
    },
    {
      "epoch": 1.7744227353463589,
      "eval_loss": 5.9223127365112305,
      "eval_runtime": 3.7344,
      "eval_samples_per_second": 66.945,
      "eval_steps_per_second": 16.87,
      "step": 250
    },
    {
      "epoch": 1.7815275310834813,
      "grad_norm": 1.764475703239441,
      "learning_rate": 4.188861985472155e-05,
      "loss": 1.1222,
      "step": 251
    },
    {
      "epoch": 1.788632326820604,
      "grad_norm": 2.0383498668670654,
      "learning_rate": 4.1646489104116224e-05,
      "loss": 1.059,
      "step": 252
    },
    {
      "epoch": 1.7957371225577266,
      "grad_norm": 2.029676914215088,
      "learning_rate": 4.14043583535109e-05,
      "loss": 1.0438,
      "step": 253
    },
    {
      "epoch": 1.802841918294849,
      "grad_norm": 1.8334102630615234,
      "learning_rate": 4.1162227602905574e-05,
      "loss": 1.0766,
      "step": 254
    },
    {
      "epoch": 1.8099467140319716,
      "grad_norm": 1.86326003074646,
      "learning_rate": 4.092009685230024e-05,
      "loss": 1.0911,
      "step": 255
    },
    {
      "epoch": 1.8170515097690942,
      "grad_norm": 2.0635340213775635,
      "learning_rate": 4.067796610169492e-05,
      "loss": 0.98,
      "step": 256
    },
    {
      "epoch": 1.8241563055062167,
      "grad_norm": 2.0353078842163086,
      "learning_rate": 4.043583535108959e-05,
      "loss": 1.0481,
      "step": 257
    },
    {
      "epoch": 1.8312611012433393,
      "grad_norm": 1.950562596321106,
      "learning_rate": 4.019370460048426e-05,
      "loss": 1.0368,
      "step": 258
    },
    {
      "epoch": 1.838365896980462,
      "grad_norm": 1.9848686456680298,
      "learning_rate": 3.9951573849878936e-05,
      "loss": 1.076,
      "step": 259
    },
    {
      "epoch": 1.8454706927175843,
      "grad_norm": 1.8926384449005127,
      "learning_rate": 3.970944309927361e-05,
      "loss": 1.0386,
      "step": 260
    },
    {
      "epoch": 1.8525754884547068,
      "grad_norm": 1.9986923933029175,
      "learning_rate": 3.9467312348668285e-05,
      "loss": 1.066,
      "step": 261
    },
    {
      "epoch": 1.8596802841918296,
      "grad_norm": 1.5582380294799805,
      "learning_rate": 3.9225181598062954e-05,
      "loss": 0.98,
      "step": 262
    },
    {
      "epoch": 1.866785079928952,
      "grad_norm": 2.023594379425049,
      "learning_rate": 3.898305084745763e-05,
      "loss": 1.0335,
      "step": 263
    },
    {
      "epoch": 1.8738898756660745,
      "grad_norm": 2.07951021194458,
      "learning_rate": 3.8740920096852304e-05,
      "loss": 0.972,
      "step": 264
    },
    {
      "epoch": 1.8809946714031973,
      "grad_norm": 1.7805190086364746,
      "learning_rate": 3.849878934624698e-05,
      "loss": 1.0637,
      "step": 265
    },
    {
      "epoch": 1.8880994671403197,
      "grad_norm": 1.928292155265808,
      "learning_rate": 3.825665859564165e-05,
      "loss": 0.8792,
      "step": 266
    },
    {
      "epoch": 1.8952042628774421,
      "grad_norm": 2.08248233795166,
      "learning_rate": 3.801452784503632e-05,
      "loss": 1.0334,
      "step": 267
    },
    {
      "epoch": 1.9023090586145648,
      "grad_norm": 2.108145236968994,
      "learning_rate": 3.7772397094431e-05,
      "loss": 1.0744,
      "step": 268
    },
    {
      "epoch": 1.9094138543516874,
      "grad_norm": 2.2863566875457764,
      "learning_rate": 3.7530266343825665e-05,
      "loss": 1.0155,
      "step": 269
    },
    {
      "epoch": 1.9165186500888098,
      "grad_norm": 1.7126070261001587,
      "learning_rate": 3.728813559322034e-05,
      "loss": 1.097,
      "step": 270
    },
    {
      "epoch": 1.9236234458259325,
      "grad_norm": 2.135889768600464,
      "learning_rate": 3.7046004842615015e-05,
      "loss": 1.0602,
      "step": 271
    },
    {
      "epoch": 1.9307282415630551,
      "grad_norm": 2.038926601409912,
      "learning_rate": 3.680387409200969e-05,
      "loss": 1.0363,
      "step": 272
    },
    {
      "epoch": 1.9378330373001775,
      "grad_norm": 1.8805986642837524,
      "learning_rate": 3.656174334140436e-05,
      "loss": 1.0282,
      "step": 273
    },
    {
      "epoch": 1.9449378330373002,
      "grad_norm": 2.1098248958587646,
      "learning_rate": 3.631961259079903e-05,
      "loss": 1.0829,
      "step": 274
    },
    {
      "epoch": 1.9520426287744228,
      "grad_norm": 1.7833491563796997,
      "learning_rate": 3.607748184019371e-05,
      "loss": 1.0172,
      "step": 275
    },
    {
      "epoch": 1.9591474245115452,
      "grad_norm": 1.7566951513290405,
      "learning_rate": 3.583535108958838e-05,
      "loss": 1.101,
      "step": 276
    },
    {
      "epoch": 1.9662522202486679,
      "grad_norm": 1.9739338159561157,
      "learning_rate": 3.559322033898305e-05,
      "loss": 1.0053,
      "step": 277
    },
    {
      "epoch": 1.9733570159857905,
      "grad_norm": 1.8095797300338745,
      "learning_rate": 3.5351089588377726e-05,
      "loss": 1.037,
      "step": 278
    },
    {
      "epoch": 1.980461811722913,
      "grad_norm": 1.9503368139266968,
      "learning_rate": 3.51089588377724e-05,
      "loss": 1.018,
      "step": 279
    },
    {
      "epoch": 1.9875666074600356,
      "grad_norm": 2.0616583824157715,
      "learning_rate": 3.486682808716707e-05,
      "loss": 1.0813,
      "step": 280
    },
    {
      "epoch": 1.9946714031971582,
      "grad_norm": 1.6979146003723145,
      "learning_rate": 3.4624697336561744e-05,
      "loss": 0.9588,
      "step": 281
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.8787713050842285,
      "learning_rate": 3.438256658595642e-05,
      "loss": 1.0318,
      "step": 282
    },
    {
      "epoch": 2.0071047957371224,
      "grad_norm": 1.6896060705184937,
      "learning_rate": 3.4140435835351094e-05,
      "loss": 0.973,
      "step": 283
    },
    {
      "epoch": 2.0142095914742453,
      "grad_norm": 1.8217474222183228,
      "learning_rate": 3.389830508474576e-05,
      "loss": 1.0286,
      "step": 284
    },
    {
      "epoch": 2.0213143872113677,
      "grad_norm": 1.7015947103500366,
      "learning_rate": 3.365617433414044e-05,
      "loss": 0.9939,
      "step": 285
    },
    {
      "epoch": 2.02841918294849,
      "grad_norm": 1.9863687753677368,
      "learning_rate": 3.341404358353511e-05,
      "loss": 0.98,
      "step": 286
    },
    {
      "epoch": 2.035523978685613,
      "grad_norm": 1.761590838432312,
      "learning_rate": 3.317191283292979e-05,
      "loss": 0.9207,
      "step": 287
    },
    {
      "epoch": 2.0426287744227354,
      "grad_norm": 1.901516079902649,
      "learning_rate": 3.2929782082324455e-05,
      "loss": 1.0244,
      "step": 288
    },
    {
      "epoch": 2.049733570159858,
      "grad_norm": 1.7907450199127197,
      "learning_rate": 3.268765133171913e-05,
      "loss": 0.9346,
      "step": 289
    },
    {
      "epoch": 2.0568383658969807,
      "grad_norm": 2.1780588626861572,
      "learning_rate": 3.2445520581113805e-05,
      "loss": 0.9527,
      "step": 290
    },
    {
      "epoch": 2.063943161634103,
      "grad_norm": 1.8411182165145874,
      "learning_rate": 3.2203389830508473e-05,
      "loss": 0.8814,
      "step": 291
    },
    {
      "epoch": 2.0710479573712255,
      "grad_norm": 1.7885006666183472,
      "learning_rate": 3.196125907990315e-05,
      "loss": 1.0579,
      "step": 292
    },
    {
      "epoch": 2.0781527531083483,
      "grad_norm": 1.8956964015960693,
      "learning_rate": 3.1719128329297823e-05,
      "loss": 0.9541,
      "step": 293
    },
    {
      "epoch": 2.0852575488454708,
      "grad_norm": 1.7386283874511719,
      "learning_rate": 3.14769975786925e-05,
      "loss": 0.9199,
      "step": 294
    },
    {
      "epoch": 2.092362344582593,
      "grad_norm": 1.87253737449646,
      "learning_rate": 3.1234866828087167e-05,
      "loss": 1.0228,
      "step": 295
    },
    {
      "epoch": 2.0994671403197156,
      "grad_norm": 2.123417854309082,
      "learning_rate": 3.099273607748184e-05,
      "loss": 1.0421,
      "step": 296
    },
    {
      "epoch": 2.1065719360568385,
      "grad_norm": 2.2708630561828613,
      "learning_rate": 3.0750605326876516e-05,
      "loss": 0.9378,
      "step": 297
    },
    {
      "epoch": 2.113676731793961,
      "grad_norm": 1.9818155765533447,
      "learning_rate": 3.050847457627119e-05,
      "loss": 1.0039,
      "step": 298
    },
    {
      "epoch": 2.1207815275310833,
      "grad_norm": 2.031585931777954,
      "learning_rate": 3.026634382566586e-05,
      "loss": 0.9616,
      "step": 299
    },
    {
      "epoch": 2.127886323268206,
      "grad_norm": 1.9849480390548706,
      "learning_rate": 3.0024213075060538e-05,
      "loss": 1.06,
      "step": 300
    },
    {
      "epoch": 2.127886323268206,
      "eval_loss": 5.964492321014404,
      "eval_runtime": 2.6666,
      "eval_samples_per_second": 93.754,
      "eval_steps_per_second": 23.626,
      "step": 300
    },
    {
      "epoch": 2.1349911190053286,
      "grad_norm": 1.8292866945266724,
      "learning_rate": 2.9782082324455206e-05,
      "loss": 0.8932,
      "step": 301
    },
    {
      "epoch": 2.142095914742451,
      "grad_norm": 2.126159191131592,
      "learning_rate": 2.9539951573849878e-05,
      "loss": 1.036,
      "step": 302
    },
    {
      "epoch": 2.149200710479574,
      "grad_norm": 1.991821527481079,
      "learning_rate": 2.9297820823244553e-05,
      "loss": 0.9546,
      "step": 303
    },
    {
      "epoch": 2.1563055062166963,
      "grad_norm": 1.8851152658462524,
      "learning_rate": 2.9055690072639224e-05,
      "loss": 0.9435,
      "step": 304
    },
    {
      "epoch": 2.1634103019538187,
      "grad_norm": 2.138429880142212,
      "learning_rate": 2.88135593220339e-05,
      "loss": 0.899,
      "step": 305
    },
    {
      "epoch": 2.1705150976909415,
      "grad_norm": 1.9105256795883179,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.97,
      "step": 306
    },
    {
      "epoch": 2.177619893428064,
      "grad_norm": 2.0981407165527344,
      "learning_rate": 2.8329297820823246e-05,
      "loss": 0.9827,
      "step": 307
    },
    {
      "epoch": 2.1847246891651864,
      "grad_norm": 1.9313169717788696,
      "learning_rate": 2.8087167070217917e-05,
      "loss": 0.9627,
      "step": 308
    },
    {
      "epoch": 2.191829484902309,
      "grad_norm": 2.1710779666900635,
      "learning_rate": 2.7845036319612596e-05,
      "loss": 0.9911,
      "step": 309
    },
    {
      "epoch": 2.1989342806394316,
      "grad_norm": 2.081637144088745,
      "learning_rate": 2.7602905569007264e-05,
      "loss": 0.9925,
      "step": 310
    },
    {
      "epoch": 2.206039076376554,
      "grad_norm": 2.0181851387023926,
      "learning_rate": 2.7360774818401942e-05,
      "loss": 0.9154,
      "step": 311
    },
    {
      "epoch": 2.213143872113677,
      "grad_norm": 2.2391750812530518,
      "learning_rate": 2.711864406779661e-05,
      "loss": 0.9922,
      "step": 312
    },
    {
      "epoch": 2.2202486678507993,
      "grad_norm": 2.1391000747680664,
      "learning_rate": 2.6876513317191282e-05,
      "loss": 1.0005,
      "step": 313
    },
    {
      "epoch": 2.2273534635879217,
      "grad_norm": 2.1485867500305176,
      "learning_rate": 2.6634382566585957e-05,
      "loss": 0.9987,
      "step": 314
    },
    {
      "epoch": 2.2344582593250446,
      "grad_norm": 2.262021780014038,
      "learning_rate": 2.639225181598063e-05,
      "loss": 0.9828,
      "step": 315
    },
    {
      "epoch": 2.241563055062167,
      "grad_norm": 1.9284793138504028,
      "learning_rate": 2.6150121065375304e-05,
      "loss": 0.9683,
      "step": 316
    },
    {
      "epoch": 2.2486678507992894,
      "grad_norm": 2.5518293380737305,
      "learning_rate": 2.5907990314769975e-05,
      "loss": 1.0469,
      "step": 317
    },
    {
      "epoch": 2.2557726465364123,
      "grad_norm": 1.8830267190933228,
      "learning_rate": 2.566585956416465e-05,
      "loss": 0.9068,
      "step": 318
    },
    {
      "epoch": 2.2628774422735347,
      "grad_norm": 2.0033960342407227,
      "learning_rate": 2.5423728813559322e-05,
      "loss": 1.0228,
      "step": 319
    },
    {
      "epoch": 2.269982238010657,
      "grad_norm": 1.9590530395507812,
      "learning_rate": 2.5181598062953997e-05,
      "loss": 0.9985,
      "step": 320
    },
    {
      "epoch": 2.2770870337477795,
      "grad_norm": 2.130444288253784,
      "learning_rate": 2.4939467312348668e-05,
      "loss": 1.0358,
      "step": 321
    },
    {
      "epoch": 2.2841918294849024,
      "grad_norm": 1.9972537755966187,
      "learning_rate": 2.4697336561743343e-05,
      "loss": 0.9752,
      "step": 322
    },
    {
      "epoch": 2.291296625222025,
      "grad_norm": 2.0860443115234375,
      "learning_rate": 2.4455205811138015e-05,
      "loss": 1.0178,
      "step": 323
    },
    {
      "epoch": 2.2984014209591472,
      "grad_norm": 2.520873546600342,
      "learning_rate": 2.421307506053269e-05,
      "loss": 0.9662,
      "step": 324
    },
    {
      "epoch": 2.30550621669627,
      "grad_norm": 2.1893320083618164,
      "learning_rate": 2.397094430992736e-05,
      "loss": 0.9791,
      "step": 325
    },
    {
      "epoch": 2.3126110124333925,
      "grad_norm": 2.2051286697387695,
      "learning_rate": 2.3728813559322036e-05,
      "loss": 0.911,
      "step": 326
    },
    {
      "epoch": 2.319715808170515,
      "grad_norm": 2.0818631649017334,
      "learning_rate": 2.3486682808716708e-05,
      "loss": 0.9201,
      "step": 327
    },
    {
      "epoch": 2.326820603907638,
      "grad_norm": 2.0052218437194824,
      "learning_rate": 2.3244552058111383e-05,
      "loss": 0.99,
      "step": 328
    },
    {
      "epoch": 2.33392539964476,
      "grad_norm": 2.0881166458129883,
      "learning_rate": 2.3002421307506054e-05,
      "loss": 0.9734,
      "step": 329
    },
    {
      "epoch": 2.3410301953818826,
      "grad_norm": 2.137723207473755,
      "learning_rate": 2.2760290556900726e-05,
      "loss": 0.9034,
      "step": 330
    },
    {
      "epoch": 2.3481349911190055,
      "grad_norm": 2.242976665496826,
      "learning_rate": 2.25181598062954e-05,
      "loss": 0.9685,
      "step": 331
    },
    {
      "epoch": 2.355239786856128,
      "grad_norm": 2.144207239151001,
      "learning_rate": 2.2276029055690073e-05,
      "loss": 1.018,
      "step": 332
    },
    {
      "epoch": 2.3623445825932503,
      "grad_norm": 2.286278009414673,
      "learning_rate": 2.2033898305084748e-05,
      "loss": 0.9457,
      "step": 333
    },
    {
      "epoch": 2.369449378330373,
      "grad_norm": 1.9013339281082153,
      "learning_rate": 2.179176755447942e-05,
      "loss": 0.9947,
      "step": 334
    },
    {
      "epoch": 2.3765541740674956,
      "grad_norm": 2.1592423915863037,
      "learning_rate": 2.1549636803874094e-05,
      "loss": 0.9483,
      "step": 335
    },
    {
      "epoch": 2.383658969804618,
      "grad_norm": 2.229505777359009,
      "learning_rate": 2.1307506053268766e-05,
      "loss": 0.9107,
      "step": 336
    },
    {
      "epoch": 2.390763765541741,
      "grad_norm": 2.033527135848999,
      "learning_rate": 2.106537530266344e-05,
      "loss": 0.996,
      "step": 337
    },
    {
      "epoch": 2.3978685612788633,
      "grad_norm": 2.341073513031006,
      "learning_rate": 2.0823244552058112e-05,
      "loss": 1.0169,
      "step": 338
    },
    {
      "epoch": 2.4049733570159857,
      "grad_norm": 2.204960823059082,
      "learning_rate": 2.0581113801452787e-05,
      "loss": 0.9702,
      "step": 339
    },
    {
      "epoch": 2.412078152753108,
      "grad_norm": 1.9607393741607666,
      "learning_rate": 2.033898305084746e-05,
      "loss": 0.9693,
      "step": 340
    },
    {
      "epoch": 2.419182948490231,
      "grad_norm": 2.1170806884765625,
      "learning_rate": 2.009685230024213e-05,
      "loss": 0.9741,
      "step": 341
    },
    {
      "epoch": 2.4262877442273534,
      "grad_norm": 2.256711006164551,
      "learning_rate": 1.9854721549636805e-05,
      "loss": 1.0039,
      "step": 342
    },
    {
      "epoch": 2.4333925399644762,
      "grad_norm": 2.122025966644287,
      "learning_rate": 1.9612590799031477e-05,
      "loss": 0.9396,
      "step": 343
    },
    {
      "epoch": 2.4404973357015987,
      "grad_norm": 2.203068971633911,
      "learning_rate": 1.9370460048426152e-05,
      "loss": 0.957,
      "step": 344
    },
    {
      "epoch": 2.447602131438721,
      "grad_norm": 2.087395191192627,
      "learning_rate": 1.9128329297820823e-05,
      "loss": 0.9944,
      "step": 345
    },
    {
      "epoch": 2.4547069271758435,
      "grad_norm": 2.0361411571502686,
      "learning_rate": 1.88861985472155e-05,
      "loss": 0.9841,
      "step": 346
    },
    {
      "epoch": 2.4618117229129663,
      "grad_norm": 1.97296142578125,
      "learning_rate": 1.864406779661017e-05,
      "loss": 0.9699,
      "step": 347
    },
    {
      "epoch": 2.4689165186500888,
      "grad_norm": 2.2020816802978516,
      "learning_rate": 1.8401937046004845e-05,
      "loss": 1.0146,
      "step": 348
    },
    {
      "epoch": 2.476021314387211,
      "grad_norm": 2.0501179695129395,
      "learning_rate": 1.8159806295399516e-05,
      "loss": 1.0244,
      "step": 349
    },
    {
      "epoch": 2.483126110124334,
      "grad_norm": 2.0399138927459717,
      "learning_rate": 1.791767554479419e-05,
      "loss": 0.8992,
      "step": 350
    },
    {
      "epoch": 2.483126110124334,
      "eval_loss": 5.961845874786377,
      "eval_runtime": 2.2345,
      "eval_samples_per_second": 111.883,
      "eval_steps_per_second": 28.195,
      "step": 350
    },
    {
      "epoch": 2.4902309058614565,
      "grad_norm": 2.1815030574798584,
      "learning_rate": 1.7675544794188863e-05,
      "loss": 1.079,
      "step": 351
    },
    {
      "epoch": 2.497335701598579,
      "grad_norm": 2.235405683517456,
      "learning_rate": 1.7433414043583535e-05,
      "loss": 0.939,
      "step": 352
    },
    {
      "epoch": 2.5044404973357017,
      "grad_norm": 2.121518135070801,
      "learning_rate": 1.719128329297821e-05,
      "loss": 1.0149,
      "step": 353
    },
    {
      "epoch": 2.511545293072824,
      "grad_norm": 2.4425816535949707,
      "learning_rate": 1.694915254237288e-05,
      "loss": 0.9025,
      "step": 354
    },
    {
      "epoch": 2.5186500888099466,
      "grad_norm": 2.1095149517059326,
      "learning_rate": 1.6707021791767556e-05,
      "loss": 1.0389,
      "step": 355
    },
    {
      "epoch": 2.5257548845470694,
      "grad_norm": 2.215442657470703,
      "learning_rate": 1.6464891041162228e-05,
      "loss": 0.9721,
      "step": 356
    },
    {
      "epoch": 2.532859680284192,
      "grad_norm": 2.0984504222869873,
      "learning_rate": 1.6222760290556903e-05,
      "loss": 1.0369,
      "step": 357
    },
    {
      "epoch": 2.5399644760213143,
      "grad_norm": 2.2687652111053467,
      "learning_rate": 1.5980629539951574e-05,
      "loss": 0.9972,
      "step": 358
    },
    {
      "epoch": 2.5470692717584367,
      "grad_norm": 2.275329113006592,
      "learning_rate": 1.573849878934625e-05,
      "loss": 1.0285,
      "step": 359
    },
    {
      "epoch": 2.5541740674955595,
      "grad_norm": 2.2368459701538086,
      "learning_rate": 1.549636803874092e-05,
      "loss": 1.0267,
      "step": 360
    },
    {
      "epoch": 2.561278863232682,
      "grad_norm": 2.208643674850464,
      "learning_rate": 1.5254237288135596e-05,
      "loss": 1.0004,
      "step": 361
    },
    {
      "epoch": 2.568383658969805,
      "grad_norm": 2.2018418312072754,
      "learning_rate": 1.5012106537530269e-05,
      "loss": 0.9426,
      "step": 362
    },
    {
      "epoch": 2.575488454706927,
      "grad_norm": 2.145456314086914,
      "learning_rate": 1.4769975786924939e-05,
      "loss": 0.9593,
      "step": 363
    },
    {
      "epoch": 2.5825932504440496,
      "grad_norm": 2.030064105987549,
      "learning_rate": 1.4527845036319612e-05,
      "loss": 0.9293,
      "step": 364
    },
    {
      "epoch": 2.589698046181172,
      "grad_norm": 2.072641372680664,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.9717,
      "step": 365
    },
    {
      "epoch": 2.596802841918295,
      "grad_norm": 2.000340700149536,
      "learning_rate": 1.4043583535108959e-05,
      "loss": 1.001,
      "step": 366
    },
    {
      "epoch": 2.6039076376554173,
      "grad_norm": 2.26119327545166,
      "learning_rate": 1.3801452784503632e-05,
      "loss": 1.0466,
      "step": 367
    },
    {
      "epoch": 2.61101243339254,
      "grad_norm": 2.12329363822937,
      "learning_rate": 1.3559322033898305e-05,
      "loss": 1.0072,
      "step": 368
    },
    {
      "epoch": 2.6181172291296626,
      "grad_norm": 1.8707364797592163,
      "learning_rate": 1.3317191283292979e-05,
      "loss": 0.9241,
      "step": 369
    },
    {
      "epoch": 2.625222024866785,
      "grad_norm": 1.9372373819351196,
      "learning_rate": 1.3075060532687652e-05,
      "loss": 1.0487,
      "step": 370
    },
    {
      "epoch": 2.6323268206039074,
      "grad_norm": 2.100309133529663,
      "learning_rate": 1.2832929782082325e-05,
      "loss": 1.0125,
      "step": 371
    },
    {
      "epoch": 2.6394316163410303,
      "grad_norm": 2.212658166885376,
      "learning_rate": 1.2590799031476998e-05,
      "loss": 0.9984,
      "step": 372
    },
    {
      "epoch": 2.6465364120781527,
      "grad_norm": 2.184279441833496,
      "learning_rate": 1.2348668280871672e-05,
      "loss": 1.0112,
      "step": 373
    },
    {
      "epoch": 2.6536412078152756,
      "grad_norm": 2.0650954246520996,
      "learning_rate": 1.2106537530266345e-05,
      "loss": 0.9405,
      "step": 374
    },
    {
      "epoch": 2.660746003552398,
      "grad_norm": 2.116259813308716,
      "learning_rate": 1.1864406779661018e-05,
      "loss": 1.0122,
      "step": 375
    },
    {
      "epoch": 2.6678507992895204,
      "grad_norm": 1.9845000505447388,
      "learning_rate": 1.1622276029055691e-05,
      "loss": 0.9422,
      "step": 376
    },
    {
      "epoch": 2.674955595026643,
      "grad_norm": 2.2809908390045166,
      "learning_rate": 1.1380145278450363e-05,
      "loss": 1.0054,
      "step": 377
    },
    {
      "epoch": 2.6820603907637657,
      "grad_norm": 2.0847902297973633,
      "learning_rate": 1.1138014527845036e-05,
      "loss": 1.0209,
      "step": 378
    },
    {
      "epoch": 2.689165186500888,
      "grad_norm": 2.243948221206665,
      "learning_rate": 1.089588377723971e-05,
      "loss": 0.9964,
      "step": 379
    },
    {
      "epoch": 2.6962699822380105,
      "grad_norm": 1.9795807600021362,
      "learning_rate": 1.0653753026634383e-05,
      "loss": 0.9735,
      "step": 380
    },
    {
      "epoch": 2.7033747779751334,
      "grad_norm": 2.447711706161499,
      "learning_rate": 1.0411622276029056e-05,
      "loss": 0.9842,
      "step": 381
    },
    {
      "epoch": 2.710479573712256,
      "grad_norm": 2.0305864810943604,
      "learning_rate": 1.016949152542373e-05,
      "loss": 0.9592,
      "step": 382
    },
    {
      "epoch": 2.717584369449378,
      "grad_norm": 1.9277368783950806,
      "learning_rate": 9.927360774818403e-06,
      "loss": 0.934,
      "step": 383
    },
    {
      "epoch": 2.7246891651865006,
      "grad_norm": 1.854414463043213,
      "learning_rate": 9.685230024213076e-06,
      "loss": 0.9669,
      "step": 384
    },
    {
      "epoch": 2.7317939609236235,
      "grad_norm": 1.7981677055358887,
      "learning_rate": 9.44309927360775e-06,
      "loss": 1.0414,
      "step": 385
    },
    {
      "epoch": 2.738898756660746,
      "grad_norm": 2.028942823410034,
      "learning_rate": 9.200968523002422e-06,
      "loss": 1.0065,
      "step": 386
    },
    {
      "epoch": 2.7460035523978688,
      "grad_norm": 2.4889843463897705,
      "learning_rate": 8.958837772397096e-06,
      "loss": 0.9495,
      "step": 387
    },
    {
      "epoch": 2.753108348134991,
      "grad_norm": 2.1639230251312256,
      "learning_rate": 8.716707021791767e-06,
      "loss": 0.96,
      "step": 388
    },
    {
      "epoch": 2.7602131438721136,
      "grad_norm": 2.3952715396881104,
      "learning_rate": 8.47457627118644e-06,
      "loss": 0.9022,
      "step": 389
    },
    {
      "epoch": 2.767317939609236,
      "grad_norm": 2.1383402347564697,
      "learning_rate": 8.232445520581114e-06,
      "loss": 0.9197,
      "step": 390
    },
    {
      "epoch": 2.774422735346359,
      "grad_norm": 2.110342502593994,
      "learning_rate": 7.990314769975787e-06,
      "loss": 0.9777,
      "step": 391
    },
    {
      "epoch": 2.7815275310834813,
      "grad_norm": 2.189117670059204,
      "learning_rate": 7.74818401937046e-06,
      "loss": 0.9071,
      "step": 392
    },
    {
      "epoch": 2.788632326820604,
      "grad_norm": 2.1858065128326416,
      "learning_rate": 7.5060532687651345e-06,
      "loss": 0.9753,
      "step": 393
    },
    {
      "epoch": 2.7957371225577266,
      "grad_norm": 2.2981889247894287,
      "learning_rate": 7.263922518159806e-06,
      "loss": 1.0317,
      "step": 394
    },
    {
      "epoch": 2.802841918294849,
      "grad_norm": 2.1879889965057373,
      "learning_rate": 7.021791767554479e-06,
      "loss": 0.9837,
      "step": 395
    },
    {
      "epoch": 2.8099467140319714,
      "grad_norm": 2.0052332878112793,
      "learning_rate": 6.779661016949153e-06,
      "loss": 0.9689,
      "step": 396
    },
    {
      "epoch": 2.8170515097690942,
      "grad_norm": 2.354448080062866,
      "learning_rate": 6.537530266343826e-06,
      "loss": 1.0361,
      "step": 397
    },
    {
      "epoch": 2.8241563055062167,
      "grad_norm": 2.0446970462799072,
      "learning_rate": 6.295399515738499e-06,
      "loss": 0.9565,
      "step": 398
    },
    {
      "epoch": 2.8312611012433395,
      "grad_norm": 2.1166422367095947,
      "learning_rate": 6.0532687651331724e-06,
      "loss": 0.975,
      "step": 399
    },
    {
      "epoch": 2.838365896980462,
      "grad_norm": 2.0646750926971436,
      "learning_rate": 5.811138014527846e-06,
      "loss": 0.9711,
      "step": 400
    },
    {
      "epoch": 2.838365896980462,
      "eval_loss": 5.966681003570557,
      "eval_runtime": 2.262,
      "eval_samples_per_second": 110.524,
      "eval_steps_per_second": 27.852,
      "step": 400
    },
    {
      "epoch": 2.8454706927175843,
      "grad_norm": 1.9168545007705688,
      "learning_rate": 5.569007263922518e-06,
      "loss": 0.9484,
      "step": 401
    },
    {
      "epoch": 2.8525754884547068,
      "grad_norm": 2.0112247467041016,
      "learning_rate": 5.326876513317191e-06,
      "loss": 0.9386,
      "step": 402
    },
    {
      "epoch": 2.8596802841918296,
      "grad_norm": 2.2338788509368896,
      "learning_rate": 5.084745762711865e-06,
      "loss": 0.9326,
      "step": 403
    },
    {
      "epoch": 2.866785079928952,
      "grad_norm": 1.7557309865951538,
      "learning_rate": 4.842615012106538e-06,
      "loss": 0.9083,
      "step": 404
    },
    {
      "epoch": 2.8738898756660745,
      "grad_norm": 2.2133748531341553,
      "learning_rate": 4.600484261501211e-06,
      "loss": 0.985,
      "step": 405
    },
    {
      "epoch": 2.8809946714031973,
      "grad_norm": 1.8518816232681274,
      "learning_rate": 4.358353510895884e-06,
      "loss": 0.9433,
      "step": 406
    },
    {
      "epoch": 2.8880994671403197,
      "grad_norm": 2.0403106212615967,
      "learning_rate": 4.116222760290557e-06,
      "loss": 0.9826,
      "step": 407
    },
    {
      "epoch": 2.895204262877442,
      "grad_norm": 2.1494858264923096,
      "learning_rate": 3.87409200968523e-06,
      "loss": 0.8738,
      "step": 408
    },
    {
      "epoch": 2.9023090586145646,
      "grad_norm": 2.2589638233184814,
      "learning_rate": 3.631961259079903e-06,
      "loss": 1.0195,
      "step": 409
    },
    {
      "epoch": 2.9094138543516874,
      "grad_norm": 1.8579702377319336,
      "learning_rate": 3.3898305084745763e-06,
      "loss": 0.9178,
      "step": 410
    },
    {
      "epoch": 2.91651865008881,
      "grad_norm": 2.117685079574585,
      "learning_rate": 3.1476997578692496e-06,
      "loss": 0.9352,
      "step": 411
    },
    {
      "epoch": 2.9236234458259327,
      "grad_norm": 2.1620755195617676,
      "learning_rate": 2.905569007263923e-06,
      "loss": 0.9053,
      "step": 412
    },
    {
      "epoch": 2.930728241563055,
      "grad_norm": 2.1317336559295654,
      "learning_rate": 2.6634382566585957e-06,
      "loss": 1.0125,
      "step": 413
    },
    {
      "epoch": 2.9378330373001775,
      "grad_norm": 2.0357706546783447,
      "learning_rate": 2.421307506053269e-06,
      "loss": 1.048,
      "step": 414
    },
    {
      "epoch": 2.9449378330373,
      "grad_norm": 2.1280293464660645,
      "learning_rate": 2.179176755447942e-06,
      "loss": 0.9992,
      "step": 415
    },
    {
      "epoch": 2.952042628774423,
      "grad_norm": 2.3383595943450928,
      "learning_rate": 1.937046004842615e-06,
      "loss": 0.9923,
      "step": 416
    },
    {
      "epoch": 2.959147424511545,
      "grad_norm": 1.8736903667449951,
      "learning_rate": 1.6949152542372882e-06,
      "loss": 0.8962,
      "step": 417
    },
    {
      "epoch": 2.966252220248668,
      "grad_norm": 1.9552888870239258,
      "learning_rate": 1.4527845036319614e-06,
      "loss": 0.9913,
      "step": 418
    },
    {
      "epoch": 2.9733570159857905,
      "grad_norm": 2.064047336578369,
      "learning_rate": 1.2106537530266345e-06,
      "loss": 0.9141,
      "step": 419
    },
    {
      "epoch": 2.980461811722913,
      "grad_norm": 1.9329217672348022,
      "learning_rate": 9.685230024213075e-07,
      "loss": 1.0113,
      "step": 420
    },
    {
      "epoch": 2.9875666074600353,
      "grad_norm": 2.104138135910034,
      "learning_rate": 7.263922518159807e-07,
      "loss": 1.0107,
      "step": 421
    },
    {
      "epoch": 2.994671403197158,
      "grad_norm": 1.8737760782241821,
      "learning_rate": 4.842615012106538e-07,
      "loss": 0.9878,
      "step": 422
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.554943084716797,
      "learning_rate": 2.421307506053269e-07,
      "loss": 0.978,
      "step": 423
    }
  ],
  "logging_steps": 1,
  "max_steps": 423,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 5000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2751474502181888e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
